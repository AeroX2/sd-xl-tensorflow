{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1190a76d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import open_clip\n",
    "#from transformers import CLIPTokenizer, CLIPTextModel\n",
    "#from safetensors import safe_open\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import math\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da17caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25b81e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"C:\\\\Users\\\\James\\\\stable-diffusion-webui\\\\models\\\\Stable-diffusion\\\\sd_xl_base_1.0.ckpt\"\n",
    "#FILENAME = \"C:\\\\Users\\\\James\\\\stable-diffusion-webui\\\\models\\\\Stable-diffusion\\\\sd_xl_base_1.0.safetensors\"\n",
    "# FILENAME = \"C:\\\\Users\\\\James\\\\Downloads\\\\v1-5-pruned.ckpt\"\n",
    "weights = torch.load(FILENAME)\n",
    "#weights_f = safe_open(FILENAME, framework=\"pt\", device=\"cpu\")\n",
    "weights = {\"state_dict\": weights}\n",
    "#for key in weights_f.keys():\n",
    "#    weights['state_dict'][key] = weights_f.get_tensor(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72f92611",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def quick_gelu(x):\n",
    "  return x * tf.sigmoid(x * 1.702)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce7eb2b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CLIPAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname):\n",
    "    super(CLIPAttention, self).__init__()\n",
    "    self.embed_dim = 768\n",
    "    self.num_heads = 12\n",
    "    self.head_dim = self.embed_dim // self.num_heads\n",
    "    self.scale = self.head_dim**-0.5\n",
    "    self.k_proj = tf.keras.layers.Dense(\n",
    "        self.embed_dim,\n",
    "        activation=None,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".k_proj.weight\"].T,\n",
    "            weights[\"state_dict\"][tname + \".k_proj.bias\"],\n",
    "        ],\n",
    "    )\n",
    "    self.v_proj = tf.keras.layers.Dense(\n",
    "        self.embed_dim,\n",
    "        activation=None,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".v_proj.weight\"].T,\n",
    "            weights[\"state_dict\"][tname + \".v_proj.bias\"],\n",
    "        ],\n",
    "    )\n",
    "    self.q_proj = tf.keras.layers.Dense(\n",
    "        self.embed_dim,\n",
    "        activation=None,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".q_proj.weight\"].T,\n",
    "            weights[\"state_dict\"][tname + \".q_proj.bias\"],\n",
    "        ],\n",
    "    )\n",
    "    self.out_proj = tf.keras.layers.Dense(\n",
    "        self.embed_dim,\n",
    "        activation=None,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".out_proj.weight\"].T,\n",
    "            weights[\"state_dict\"][tname + \".out_proj.bias\"],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "  def _shape(self, tensor, seq_len: int, bsz: int):\n",
    "    a = tf.reshape(tensor, (bsz, seq_len, self.num_heads, self.head_dim))\n",
    "    # bs , n_head , seq_len , head_dim\n",
    "    return tf.keras.layers.Permute((2, 1, 3))(a)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    [hidden_states, causal_attention_mask] = inputs\n",
    "    bsz, tgt_len, embed_dim = hidden_states.shape\n",
    "    query_states = self.q_proj(hidden_states) * self.scale\n",
    "    key_states = self._shape(self.k_proj(hidden_states), tgt_len, -1)\n",
    "    value_states = self._shape(self.v_proj(hidden_states), tgt_len, -1)\n",
    "\n",
    "    proj_shape = (-1, tgt_len, self.head_dim)\n",
    "    query_states = self._shape(query_states, tgt_len, -1)\n",
    "    query_states = tf.reshape(query_states, proj_shape)\n",
    "    key_states = tf.reshape(key_states, proj_shape)\n",
    "\n",
    "    src_len = tgt_len\n",
    "    value_states = tf.reshape(value_states, proj_shape)\n",
    "    attn_weights = query_states @ tf.keras.layers.Permute(\n",
    "        (2, 1))(key_states)\n",
    "\n",
    "    attn_weights = tf.reshape(\n",
    "        attn_weights, (-1, self.num_heads, tgt_len, src_len))\n",
    "    attn_weights = attn_weights + causal_attention_mask\n",
    "    attn_weights = tf.reshape(attn_weights, (-1, tgt_len, src_len))\n",
    "\n",
    "    attn_weights = tf.nn.softmax(attn_weights)\n",
    "    attn_output = attn_weights @ value_states\n",
    "\n",
    "    attn_output = tf.reshape(\n",
    "        attn_output, (-1, self.num_heads, tgt_len, self.head_dim)\n",
    "    )\n",
    "    attn_output = tf.keras.layers.Permute((2, 1, 3))(attn_output)\n",
    "    attn_output = tf.reshape(attn_output, (-1, tgt_len, embed_dim))\n",
    "\n",
    "    attn_output = self.out_proj(attn_output)\n",
    "    return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0959b341-3bbe-48af-b03d-d67be1e56c86",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class OpenCLIPAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname):\n",
    "    super(OpenCLIPAttention, self).__init__()\n",
    "    self.embed_dim = 1280\n",
    "    self.num_heads = 12\n",
    "    self.head_dim = self.embed_dim // self.num_heads\n",
    "    self.scale = self.head_dim**-0.5\n",
    "\n",
    "    in_proj_weight = weights[\"state_dict\"][tname + \".in_proj_weight\"]\n",
    "    in_proj_bias = weights[\"state_dict\"][tname + \".in_proj_bias\"]\n",
    "\n",
    "    q_weight, k_weight, v_weight = tf.split(\n",
    "        in_proj_weight, [self.embed_dim, self.embed_dim, self.embed_dim]\n",
    "    )\n",
    "    q_bias, k_bias, v_bias = tf.split(\n",
    "        in_proj_bias, [self.embed_dim, self.embed_dim, self.embed_dim]\n",
    "    )\n",
    "\n",
    "    self.k_proj = tf.keras.layers.Dense(\n",
    "        self.embed_dim, activation=None, weights=[k_weight, k_bias]\n",
    "    )\n",
    "    self.v_proj = tf.keras.layers.Dense(\n",
    "        self.embed_dim, activation=None, weights=[v_weight, q_bias]\n",
    "    )\n",
    "    self.q_proj = tf.keras.layers.Dense(\n",
    "        self.embed_dim, activation=None, weights=[q_weight, q_bias]\n",
    "    )\n",
    "    self.out_proj = tf.keras.layers.Dense(\n",
    "        self.embed_dim,\n",
    "        activation=None,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".out_proj.weight\"].T,\n",
    "            weights[\"state_dict\"][tname + \".out_proj.bias\"],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "  def _shape(self, tensor, seq_len: int, bsz: int):\n",
    "    a = tf.reshape(tensor, (bsz, seq_len, self.num_heads, self.head_dim))\n",
    "    # bs , n_head , seq_len , head_dim\n",
    "    return tf.keras.layers.Permute((2, 1, 3))(a)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    [hidden_states, causal_attention_mask] = inputs\n",
    "    bsz, tgt_len, embed_dim = hidden_states.shape\n",
    "    query_states = self.q_proj(hidden_states) * self.scale\n",
    "    key_states = self._shape(self.k_proj(hidden_states), tgt_len, -1)\n",
    "    value_states = self._shape(self.v_proj(hidden_states), tgt_len, -1)\n",
    "\n",
    "    proj_shape = (-1, tgt_len, self.head_dim)\n",
    "    query_states = self._shape(query_states, tgt_len, -1)\n",
    "    query_states = tf.reshape(query_states, proj_shape)\n",
    "    key_states = tf.reshape(key_states, proj_shape)\n",
    "\n",
    "    src_len = tgt_len\n",
    "    value_states = tf.reshape(value_states, proj_shape)\n",
    "    attn_weights = query_states @ tf.keras.layers.Permute(\n",
    "        (2, 1))(key_states)\n",
    "\n",
    "    attn_weights = tf.reshape(\n",
    "        attn_weights, (-1, self.num_heads, tgt_len, src_len))\n",
    "    attn_weights = attn_weights + causal_attention_mask\n",
    "    attn_weights = tf.reshape(attn_weights, (-1, tgt_len, src_len))\n",
    "\n",
    "    attn_weights = tf.nn.softmax(attn_weights)\n",
    "    attn_output = attn_weights @ value_states\n",
    "\n",
    "    attn_output = tf.reshape(\n",
    "        attn_output, (-1, self.num_heads, tgt_len, self.head_dim)\n",
    "    )\n",
    "    attn_output = tf.keras.layers.Permute((2, 1, 3))(attn_output)\n",
    "    attn_output = tf.reshape(attn_output, (-1, tgt_len, embed_dim))\n",
    "\n",
    "    attn_output = self.out_proj(attn_output)\n",
    "    return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82bec30f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CLIPMLP(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname):\n",
    "    super(CLIPMLP, self).__init__()\n",
    "    self.fc1 = tf.keras.layers.Dense(\n",
    "        3072,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".fc1.weight\"].T,\n",
    "            weights[\"state_dict\"][tname + \".fc1.bias\"],\n",
    "        ],\n",
    "    )\n",
    "    self.fc2 = tf.keras.layers.Dense(\n",
    "        768,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".fc2.weight\"].T,\n",
    "            weights[\"state_dict\"][tname + \".fc2.bias\"],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "  def __call__(self, hidden_states):\n",
    "    hidden_states = self.fc1(hidden_states)\n",
    "    hidden_states = quick_gelu(hidden_states)\n",
    "    hidden_states = self.fc2(hidden_states)\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cf73f23-400f-45cd-a838-752f52060ba2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class OpenCLIPMLP(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname):\n",
    "    super(OpenCLIPMLP, self).__init__()\n",
    "    self.c_fc = tf.keras.layers.Dense(\n",
    "        3072,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".c_fc.weight\"].T,\n",
    "            weights[\"state_dict\"][tname + \".c_fc.bias\"],\n",
    "        ],\n",
    "    )\n",
    "    self.c_proj = tf.keras.layers.Dense(\n",
    "        768,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".c_proj.weight\"].T,\n",
    "            weights[\"state_dict\"][tname + \".c_proj.bias\"],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "  def __call__(self, hidden_states):\n",
    "    hidden_states = self.c_fc(hidden_states)\n",
    "    hidden_states = gelu(hidden_states)\n",
    "    hidden_states = self.c_proj(hidden_states)\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99437cb8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CLIPEncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname):\n",
    "    super(CLIPEncoderLayer, self).__init__()\n",
    "    self.self_attn = CLIPAttention(tname + \".self_attn\")\n",
    "    self.layer_norm1 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-5,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".layer_norm1.weight\"],\n",
    "            weights[\"state_dict\"][tname + \".layer_norm1.bias\"],\n",
    "        ],\n",
    "    )\n",
    "    self.mlp = CLIPMLP(tname + \".mlp\")\n",
    "    self.layer_norm2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-5,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".layer_norm2.weight\"],\n",
    "            weights[\"state_dict\"][tname + \".layer_norm2.bias\"],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    [hidden_states, causal_attention_mask] = inputs\n",
    "    residual = hidden_states\n",
    "\n",
    "    hidden_states = self.layer_norm1(hidden_states)\n",
    "    hidden_states = self.self_attn([hidden_states, causal_attention_mask])\n",
    "    hidden_states = residual + hidden_states\n",
    "\n",
    "    residual = hidden_states\n",
    "    hidden_states = self.layer_norm2(hidden_states)\n",
    "    hidden_states = self.mlp(hidden_states)\n",
    "\n",
    "    hidden_states = residual + hidden_states\n",
    "\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "292cfca2-f1a6-4074-bf2c-9f503814596b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class OpenCLIPEncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname):\n",
    "    super(OpenCLIPEncoderLayer, self).__init__()\n",
    "    self.self_attn = OpenCLIPAttention(tname + \".attn\")\n",
    "    self.mlp = OpenCLIPMLP(tname + \".mlp\")\n",
    "\n",
    "    # ln_1 or attn_ln\n",
    "    self.layer_norm1 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-5,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".ln_1.weight\"],\n",
    "            weights[\"state_dict\"][tname + \".ln_1.bias\"],\n",
    "        ],\n",
    "    )\n",
    "    # ln_2 or mlp_ln\n",
    "    self.layer_norm2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-5,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".ln_2.weight\"],\n",
    "            weights[\"state_dict\"][tname + \".ln_2.bias\"],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    [hidden_states, causal_attention_mask] = inputs\n",
    "    residual = hidden_states\n",
    "\n",
    "    hidden_states = self.layer_norm1(hidden_states)\n",
    "    hidden_states = self.self_attn([hidden_states, causal_attention_mask])\n",
    "    hidden_states = residual + hidden_states\n",
    "\n",
    "    residual = hidden_states\n",
    "    hidden_states = self.layer_norm2(hidden_states)\n",
    "    hidden_states = self.mlp(hidden_states)\n",
    "\n",
    "    hidden_states = residual + hidden_states\n",
    "\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad2012e3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CLIPEncoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname, openclip=False):\n",
    "    super(CLIPEncoder, self).__init__()\n",
    "    if openclip:\n",
    "      self.layers = [\n",
    "          OpenCLIPEncoderLayer(tname + \".resblocks.%d\" % i) for i in range(12)\n",
    "      ]\n",
    "    else:\n",
    "      self.layers = [\n",
    "          CLIPEncoderLayer(tname + \".layers.%d\" % i) for i in range(12)\n",
    "      ]\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    [hidden_states, causal_attention_mask] = inputs\n",
    "    for i, l in enumerate(self.layers):\n",
    "      hidden_states = l([hidden_states, causal_attention_mask])\n",
    "    return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49911587",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CLIPTextEmbeddings(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname, openclip=False, n_words=77):\n",
    "    super(CLIPTextEmbeddings, self).__init__()\n",
    "\n",
    "    token_emb_W = weights[\"state_dict\"][tname + \".token_embedding.weight\"]\n",
    "    self.token_embedding_layer = tf.keras.layers.Embedding(\n",
    "        49408,\n",
    "        1280 if openclip else 768,\n",
    "        name=\"token_embedding\",\n",
    "        weights=[token_emb_W],\n",
    "    )\n",
    "\n",
    "    pos_emb_W = weights[\"state_dict\"][\n",
    "        tname\n",
    "        + (\".positional_embedding\" if openclip else \".position_embedding.weight\")\n",
    "    ]\n",
    "    self.position_embedding_layer = tf.keras.layers.Embedding(\n",
    "        n_words,\n",
    "        1280 if openclip else 768,\n",
    "        name=\"position_embedding\",\n",
    "        weights=[pos_emb_W],\n",
    "    )\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    [input_ids, position_ids] = inputs\n",
    "    word_embeddings = self.token_embedding_layer(input_ids)\n",
    "    position_embeddings = self.position_embedding_layer(position_ids)\n",
    "    return word_embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22bc7cf1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CLIPTextTransformer(tf.keras.models.Model):\n",
    "  def __init__(self, tname, openclip=False, n_words=77):\n",
    "    super(CLIPTextTransformer, self).__init__()\n",
    "    # Contains both token and position embedding\n",
    "    self.embeddings = CLIPTextEmbeddings(\n",
    "        tname + (\"\" if openclip else \".embeddings\"), openclip, n_words=n_words\n",
    "    )\n",
    "\n",
    "    # Corresponds to blocks\n",
    "    self.encoder = CLIPEncoder(\n",
    "        tname + (\".transformer\" if openclip else \".encoder\"), openclip\n",
    "    )\n",
    "\n",
    "    self.final_layer_norm = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-5,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][\n",
    "                tname\n",
    "                + (\".ln_final.weight\" if openclip else \".final_layer_norm.weight\")\n",
    "            ],\n",
    "            weights[\"state_dict\"][\n",
    "                tname +\n",
    "                (\".ln_final.bias\" if openclip else \".final_layer_norm.bias\")\n",
    "            ],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    if openclip:\n",
    "      self.text_projection = tf.keras.layers.Dense(\n",
    "          512,\n",
    "          weights=[\n",
    "              weights[\"state_dict\"][tname + \".text_projection.weight\"].T,\n",
    "              weights[\"state_dict\"][tname + \".text_projection.bias\"],\n",
    "          ],\n",
    "      )\n",
    "    else:\n",
    "      self.text_projection = None\n",
    "\n",
    "    self.causal_attention_mask = tf.constant(\n",
    "        np.triu(np.ones((1, 1, 77, 77), dtype=np.float32) * -np.inf, k=1)\n",
    "    )\n",
    "\n",
    "  def __call__(self, inputs, training=False):\n",
    "    [input_ids, inp_position_ids] = inputs\n",
    "    x = self.embeddings([input_ids, inp_position_ids])\n",
    "    x = self.encoder([x, self.causal_attention_mask])\n",
    "\n",
    "    # TODO: figure out the format of text\n",
    "    # if self.text_projection is not None:\n",
    "    # eot_indices = tf.argmax(text, axis=1)\n",
    "    # normed = self.final_layer_norm(x)\n",
    "    # o = tf.squeeze(tf.gather(normed, eot_indices, axis=1), axis=1)\n",
    "\n",
    "    # t_proj = self.text_projection.val()\n",
    "    #    pooled = o #tf.matmul(o, t_proj)\n",
    "    # else:\n",
    "    #    pooled = o\n",
    "\n",
    "    return self.final_layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fad6082",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_conv2d(tname, in_channels, out_channels, kernel_size, padding=0, stride=1):\n",
    "  W = weights[\"state_dict\"][tname + \".weight\"].numpy()\n",
    "  if len(W.shape) > 2:\n",
    "    W = np.transpose(W, (2, 3, 1, 0))\n",
    "  else:\n",
    "    # raise Exception(\"Sigh....\")\n",
    "    W = np.transpose(W)\n",
    "  assert in_channels == W.shape[-2]\n",
    "  assert out_channels == W.shape[-1]\n",
    "  b = weights[\"state_dict\"][tname + \".bias\"]\n",
    "\n",
    "  r = tf.keras.models.Sequential(\n",
    "      [\n",
    "          tf.keras.layers.ZeroPadding2D(\n",
    "              padding=(padding, padding), data_format=None),\n",
    "          tf.keras.layers.Conv2D(\n",
    "              out_channels, kernel_size, strides=(\n",
    "                  stride, stride), weights=[W, b]\n",
    "          ),\n",
    "      ]\n",
    "  )\n",
    "  return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e52b28f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_seq(x, layers):\n",
    "  for l in layers:\n",
    "    x = l(x)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af24d9fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# not to be confused with ResnetBlock\n",
    "class ResBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname, channels, emb_channels, out_channels):\n",
    "    super(ResBlock, self).__init__()\n",
    "    self.in_layers = [\n",
    "        tf.keras.layers.GroupNormalization(\n",
    "            epsilon=1e-5,\n",
    "            weights=[\n",
    "                weights[\"state_dict\"][tname + \".in_layers.0.weight\"],\n",
    "                weights[\"state_dict\"][tname + \".in_layers.0.bias\"],\n",
    "            ],\n",
    "        ),\n",
    "        tf.keras.activations.swish,\n",
    "        get_conv2d(tname + \".in_layers.2\", channels,\n",
    "                   out_channels, 3, padding=1),\n",
    "    ]\n",
    "    self.emb_layers = [\n",
    "        tf.keras.activations.swish,\n",
    "        tf.keras.layers.Dense(\n",
    "            out_channels,\n",
    "            weights=[\n",
    "                weights[\"state_dict\"][tname + \".emb_layers.1.weight\"].T,\n",
    "                weights[\"state_dict\"][tname + \".emb_layers.1.bias\"],\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    "    self.out_layers = [\n",
    "        tf.keras.layers.GroupNormalization(\n",
    "            epsilon=1e-5,\n",
    "            weights=[\n",
    "                weights[\"state_dict\"][tname + \".out_layers.0.weight\"],\n",
    "                weights[\"state_dict\"][tname + \".out_layers.0.bias\"],\n",
    "            ],\n",
    "        ),\n",
    "        tf.keras.activations.swish,\n",
    "        lambda x: x,\n",
    "        get_conv2d(\n",
    "            tname + \".out_layers.3\", out_channels, out_channels, 3, padding=1\n",
    "        ),\n",
    "    ]\n",
    "    self.skip_connection = (\n",
    "        get_conv2d(tname + \".skip_connection\", channels, out_channels, 1)\n",
    "        if channels != out_channels\n",
    "        else lambda x: x\n",
    "    )\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    x, emb = inputs\n",
    "    h = apply_seq(x, self.in_layers)\n",
    "    emb_out = apply_seq(emb, self.emb_layers)\n",
    "    h = h + emb_out[:, None, None]\n",
    "    h = apply_seq(h, self.out_layers)\n",
    "    ret = self.skip_connection(x) + h\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ed9c0f9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def td_dot(a, b):\n",
    "  assert len(a.shape) == 4\n",
    "  assert len(a.shape) == 4\n",
    "  assert a.shape[0] == b.shape[0]\n",
    "  assert b.shape[1] == a.shape[1]\n",
    "  aa = tf.reshape(a, (-1, a.shape[2], a.shape[3]))\n",
    "  bb = tf.reshape(b, (-1, b.shape[2], b.shape[3]))\n",
    "  cc = tf.keras.backend.batch_dot(aa, bb)\n",
    "  c = tf.reshape(cc, (-1, a.shape[1], cc.shape[1], cc.shape[2]))\n",
    "  return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e29258b8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def gelu(self):\n",
    "  return (\n",
    "      0.5\n",
    "      * self\n",
    "      * (\n",
    "          1\n",
    "          + tf.keras.activations.tanh(\n",
    "              self * 0.7978845608 * (1 + 0.044715 * self * self)\n",
    "          )\n",
    "      )\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93f9beb0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GEGLU(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname, dim_in, dim_out):\n",
    "    super(GEGLU, self).__init__()\n",
    "\n",
    "    self.proj = tf.keras.layers.Dense(\n",
    "        dim_out * 2,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".proj.weight\"].T,\n",
    "            weights[\"state_dict\"][tname + \".proj.bias\"],\n",
    "        ],\n",
    "    )\n",
    "    self.dim_out = dim_out\n",
    "\n",
    "  def __call__(self, x):\n",
    "    xp = self.proj(x)\n",
    "    x, gate = xp[..., : self.dim_out], xp[..., self.dim_out:]\n",
    "    ans = x * gelu(gate)\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53c692a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname, dim, mult=4):\n",
    "    super(FeedForward, self).__init__()\n",
    "    self.net = [\n",
    "        GEGLU(tname + \".net.0\", dim, dim * mult),\n",
    "        lambda x: x,\n",
    "        tf.keras.layers.Dense(\n",
    "            dim,\n",
    "            weights=[\n",
    "                weights[\"state_dict\"][tname + \".net.2.weight\"].T,\n",
    "                weights[\"state_dict\"][tname + \".net.2.bias\"],\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return apply_seq(x, self.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d9c530-445c-4cda-a6be-de69935ea257",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname, query_dim, context_dim, n_heads, d_head):\n",
    "    super(CrossAttention, self).__init__()\n",
    "    self.to_q = tf.keras.layers.Dense( n_heads*d_head,use_bias=False ,weights=[weights['state_dict'][tname+\".to_q.weight\"].T  ])\n",
    "    self.to_k = tf.keras.layers.Dense( n_heads*d_head,use_bias=False ,weights=[weights['state_dict'][tname+\".to_k.weight\"].T  ])\n",
    "    self.to_v = tf.keras.layers.Dense( n_heads*d_head,use_bias=False ,weights=[weights['state_dict'][tname+\".to_v.weight\"].T  ])\n",
    "    self.scale = d_head ** -0.5\n",
    "    self.num_heads = n_heads\n",
    "    self.head_size = d_head\n",
    "    self.to_out = [tf.keras.layers.Dense( n_heads*d_head ,weights=[weights['state_dict'][tname+\".to_out.0.weight\"].T , weights['state_dict'][tname+\".to_out.0.bias\"] ])]\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    assert type(inputs) is list\n",
    "    if len(inputs) == 1:\n",
    "        inputs = inputs + [None]\n",
    "    [x , context] = inputs\n",
    "    context = x if context is None else context\n",
    "    q,k,v = self.to_q(x), self.to_k(context), self.to_v(context)\n",
    "    assert len(x.shape) == 3\n",
    "    q = tf.reshape(q , ( -1 , x.shape[1], self.num_heads, self.head_size) )\n",
    "    k = tf.reshape(k , ( -1 , context.shape[1] , self.num_heads, self.head_size) )\n",
    "    v = tf.reshape(v, ( -1 , context.shape[1] , self.num_heads, self.head_size) )\n",
    "    \n",
    "    q = tf.keras.layers.Permute((2,1,3))(q)  # (bs, num_heads, time, head_size)\n",
    "    k = tf.keras.layers.Permute((2,3,1))(k)  # (bs, num_heads, head_size, time)\n",
    "    v = tf.keras.layers.Permute((2,1,3))(v)# (bs, num_heads, time, head_size)\n",
    "    \n",
    "    score = td_dot(q,k) * self.scale\n",
    "    weights = tf.keras.activations.softmax(score)                   # (bs, num_heads, time, time)\n",
    "    attention = td_dot(weights,v)  \n",
    "    attention = tf.keras.layers.Permute((2,1,3))(attention) # (bs, time, num_heads, head_size)\n",
    "\n",
    "    h_ = tf.reshape(attention, (-1 ,x.shape[1] , self.num_heads * self.head_size))\n",
    "    return apply_seq(h_ , self.to_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "13237bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTransformerBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname, dim, context_dim, n_heads, d_head):\n",
    "    super(BasicTransformerBlock, self).__init__()\n",
    "    self.attn1 = CrossAttention(\n",
    "        tname + \".attn1\", dim, dim, n_heads, d_head)\n",
    "    self.ff = FeedForward(tname + \".ff\", dim)\n",
    "    self.attn2 = CrossAttention(\n",
    "        tname + \".attn2\", dim, context_dim, n_heads, d_head)\n",
    "    self.norm1 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-5,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".norm1.weight\"],\n",
    "            weights[\"state_dict\"][tname + \".norm1.bias\"],\n",
    "        ],\n",
    "    )\n",
    "    self.norm2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-5,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".norm2.weight\"],\n",
    "            weights[\"state_dict\"][tname + \".norm2.bias\"],\n",
    "        ],\n",
    "    )\n",
    "    self.norm3 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-5,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".norm3.weight\"],\n",
    "            weights[\"state_dict\"][tname + \".norm3.bias\"],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    [x, context] = inputs\n",
    "\n",
    "    x = self.attn1([self.norm1(x)]) + x\n",
    "    x = self.attn2([self.norm2(x), context]) + x\n",
    "    x = self.ff(self.norm3(x)) + x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5b2cebc5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class SpatialTransformer(tf.keras.layers.Layer):\n",
    "  def __init__(\n",
    "      self, tname, depth, use_linear, channels, context_dim, n_heads, d_head\n",
    "  ):\n",
    "    super(SpatialTransformer, self).__init__()\n",
    "    self.norm = tf.keras.layers.GroupNormalization(\n",
    "        epsilon=1e-5,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".norm.weight\"],\n",
    "            weights[\"state_dict\"][tname + \".norm.bias\"],\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    self.use_linear = use_linear\n",
    "    assert channels == n_heads * d_head\n",
    "    if use_linear:\n",
    "      self.proj_in = tf.keras.layers.Dense(\n",
    "          channels,\n",
    "          weights=[\n",
    "              weights[\"state_dict\"][tname + \".proj_in.weight\"].T,\n",
    "              weights[\"state_dict\"][tname + \".proj_in.bias\"],\n",
    "          ],\n",
    "      )\n",
    "    else:\n",
    "      self.proj_in = get_conv2d(\n",
    "          tname + \".proj_in\", channels, n_heads * d_head, 1)\n",
    "\n",
    "    print(depth)\n",
    "    self.transformer_blocks = [\n",
    "        BasicTransformerBlock(\n",
    "            tname + \".transformer_blocks.0\", channels, context_dim, n_heads, d_head\n",
    "        )\n",
    "        for d in range(depth)\n",
    "    ]\n",
    "\n",
    "    if use_linear:\n",
    "      self.proj_out = tf.keras.layers.Dense(\n",
    "          channels,\n",
    "          weights=[\n",
    "              weights[\"state_dict\"][tname + \".proj_out.weight\"].T,\n",
    "              weights[\"state_dict\"][tname + \".proj_out.bias\"],\n",
    "          ],\n",
    "      )\n",
    "    else:\n",
    "      self.proj_out = get_conv2d(\n",
    "          tname + \".proj_out\", n_heads * d_head, channels, 1\n",
    "      )\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    [x, context] = inputs\n",
    "    b, h, w, c = x.shape\n",
    "    x_in = x\n",
    "    x = self.norm(x)\n",
    "    if not self.use_linear:\n",
    "      x = self.proj_in(x)\n",
    "    x = tf.reshape(x, (-1, h * w, c))\n",
    "    if self.use_linear:\n",
    "      x = self.proj_in(x)\n",
    "    for block in self.transformer_blocks:\n",
    "      x = block([x, context])\n",
    "\n",
    "    if self.use_linear:\n",
    "      x = self.proj_out(x)\n",
    "    x = tf.reshape(x, (-1, h, w, c))\n",
    "    if not self.use_linear:\n",
    "      x = self.proj_out(x)\n",
    "    ret = x + x_in\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd5beb4a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Downsample(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname, channels):\n",
    "    super(Downsample, self).__init__()\n",
    "    self.op = get_conv2d(tname + \".op\", channels,\n",
    "                         channels, 3, stride=2, padding=1)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return self.op(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d7f08083",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Upsample(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname, channels):\n",
    "    super(Upsample, self).__init__()\n",
    "    self.conv = get_conv2d(tname + \".conv\", channels,\n",
    "                           channels, 3, padding=1)\n",
    "    self.ups = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = self.ups(x)\n",
    "    return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c2bb55f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class AttnBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname, in_channels):\n",
    "    super(AttnBlock, self).__init__()\n",
    "    self.norm = tf.keras.layers.GroupNormalization(\n",
    "        epsilon=1e-5,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".norm.weight\"],\n",
    "            weights[\"state_dict\"][tname + \".norm.bias\"],\n",
    "        ],\n",
    "    )\n",
    "    self.q = get_conv2d(tname + \".q\", in_channels, in_channels, 1)\n",
    "    self.k = get_conv2d(tname + \".k\", in_channels, in_channels, 1)\n",
    "    self.v = get_conv2d(tname + \".v\", in_channels, in_channels, 1)\n",
    "    self.proj_out = get_conv2d(\n",
    "        tname + \".proj_out\", in_channels, in_channels, 1)\n",
    "\n",
    "  # copied from AttnBlock in ldm repo\n",
    "  def __call__(self, x):\n",
    "    h_ = self.norm(x)\n",
    "    q, k, v = self.q(h_), self.k(h_), self.v(h_)\n",
    "\n",
    "    # compute attention\n",
    "    b, h, w, c = q.shape\n",
    "    q = tf.reshape(q, (-1, h * w, c))  # b,hw,c\n",
    "    k = tf.keras.layers.Permute((3, 1, 2))(k)\n",
    "    k = tf.reshape(k, (-1, c, h * w))  # b,c,hw\n",
    "    w_ = q @ k\n",
    "    w_ = w_ * (c ** (-0.5))\n",
    "    w_ = tf.keras.activations.softmax(w_)\n",
    "\n",
    "    # attend to values\n",
    "    v = tf.keras.layers.Permute((3, 1, 2))(v)\n",
    "    v = tf.reshape(v, (-1, c, h * w))\n",
    "    w_ = tf.keras.layers.Permute((2, 1))(w_)\n",
    "    h_ = v @ w_\n",
    "    h_ = tf.keras.layers.Permute((2, 1))(h_)\n",
    "    h_ = tf.reshape(h_, (-1, h, w, c))\n",
    "    return x + self.proj_out(h_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9a4b791",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ResnetBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname, in_channels, out_channels=None):\n",
    "    super(ResnetBlock, self).__init__()\n",
    "    self.norm1 = tf.keras.layers.GroupNormalization(\n",
    "        epsilon=1e-5,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".norm1.weight\"],\n",
    "            weights[\"state_dict\"][tname + \".norm1.bias\"],\n",
    "        ],\n",
    "    )\n",
    "    self.conv1 = get_conv2d(\n",
    "        tname + \".conv1\", in_channels, out_channels, 3, padding=1\n",
    "    )\n",
    "    self.norm2 = tf.keras.layers.GroupNormalization(\n",
    "        epsilon=1e-5,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".norm2.weight\"],\n",
    "            weights[\"state_dict\"][tname + \".norm2.bias\"],\n",
    "        ],\n",
    "    )\n",
    "    self.conv2 = get_conv2d(\n",
    "        tname + \".conv2\", out_channels, out_channels, 3, padding=1\n",
    "    )\n",
    "    self.nin_shortcut = (\n",
    "        get_conv2d(tname + \".nin_shortcut\", in_channels, out_channels, 1)\n",
    "        if in_channels != out_channels\n",
    "        else lambda x: x\n",
    "    )\n",
    "\n",
    "  def __call__(self, x):\n",
    "    h = self.conv1(tf.keras.activations.swish(self.norm1(x)))\n",
    "    h = self.conv2(tf.keras.activations.swish(self.norm2(h)))\n",
    "    return self.nin_shortcut(x) + h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bdd1bf15",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Mid(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname, block_in):\n",
    "    super(Mid, self).__init__()\n",
    "    self.block_1 = ResnetBlock(tname + \".block_1\", block_in, block_in)\n",
    "    self.attn_1 = AttnBlock(tname + \".attn_1\", block_in)\n",
    "    self.block_2 = ResnetBlock(tname + \".block_2\", block_in, block_in)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return apply_seq(x, [self.block_1, self.attn_1, self.block_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fc3e0855",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.models.Model):\n",
    "  def __init__(self, tname):\n",
    "    super(Decoder, self).__init__()\n",
    "    sz = [(128, 256), (256, 512), (512, 512), (512, 512)]\n",
    "    self.conv_in = get_conv2d(tname + \".conv_in\", 4, 512, 3, padding=1)\n",
    "    self.mid = Mid(tname + \".mid\", 512)\n",
    "    self.upp = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "\n",
    "    self.post_quant_conv = get_conv2d(\n",
    "        \"first_stage_model.post_quant_conv\", 4, 4, 1)\n",
    "\n",
    "    arr = []\n",
    "    for i, s in enumerate(sz):\n",
    "      arr.append(\n",
    "          {\n",
    "              \"block\": [\n",
    "                  ResnetBlock(tname + \".up.%d.block.0\" % i, s[1], s[0]),\n",
    "                  ResnetBlock(tname + \".up.%d.block.1\" % i, s[0], s[0]),\n",
    "                  ResnetBlock(tname + \".up.%d.block.2\" % i, s[0], s[0]),\n",
    "              ]\n",
    "          }\n",
    "      )\n",
    "      if i != 0:\n",
    "        arr[-1][\"upsample\"] = {\n",
    "            \"conv\": get_conv2d(\n",
    "                tname + \".up.%d.upsample.conv\" % i, s[0], s[0], 3, padding=1\n",
    "            )\n",
    "        }\n",
    "    self.up = arr\n",
    "\n",
    "    self.norm_out = tf.keras.layers.GroupNormalization(\n",
    "        epsilon=1e-5,\n",
    "        weights=[\n",
    "            weights[\"state_dict\"][tname + \".norm_out.weight\"],\n",
    "            weights[\"state_dict\"][tname + \".norm_out.bias\"],\n",
    "        ],\n",
    "    )\n",
    "    self.conv_out = get_conv2d(tname + \".conv_out\", 128, 3, 3, padding=1)\n",
    "\n",
    "  def __call__(self, x, training=False):\n",
    "    x = self.post_quant_conv(1 / 0.18215 * x)\n",
    "\n",
    "    x = self.conv_in(x)\n",
    "    x = self.mid(x)\n",
    "\n",
    "    for l in self.up[::-1]:\n",
    "      for b in l[\"block\"]:\n",
    "        x = b(x)\n",
    "      if \"upsample\" in l:\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html ?\n",
    "        bs, c, py, px = x.shape\n",
    "        x = self.upp(x)\n",
    "        x = l[\"upsample\"][\"conv\"](x)\n",
    "\n",
    "    return self.conv_out(tf.keras.activations.swish(self.norm_out(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8717414",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class UNetModel(tf.keras.models.Model):\n",
    "  def __init__(\n",
    "      self,\n",
    "      tname,\n",
    "      in_channels,\n",
    "      out_channels,\n",
    "      model_channels,\n",
    "      attention_resolutions,\n",
    "      num_res_blocks,\n",
    "      channel_mult,\n",
    "      num_heads,\n",
    "      num_head_channels,\n",
    "      use_linear_in_transformer,\n",
    "      transformer_depth,\n",
    "      context_dim,\n",
    "  ):\n",
    "    super(UNetModel, self).__init__()\n",
    "    time_embed_dim = model_channels * 4\n",
    "    self.time_embed = [\n",
    "        tf.keras.layers.Dense(\n",
    "            time_embed_dim,\n",
    "            weights=[\n",
    "                weights[\"state_dict\"][tname + \".time_embed.0.weight\"].T,\n",
    "                weights[\"state_dict\"][tname + \".time_embed.0.bias\"],\n",
    "            ],\n",
    "        ),\n",
    "        tf.keras.activations.swish,\n",
    "        tf.keras.layers.Dense(\n",
    "            time_embed_dim,\n",
    "            weights=[\n",
    "                weights[\"state_dict\"][tname + \".time_embed.2.weight\"].T,\n",
    "                weights[\"state_dict\"][tname + \".time_embed.2.bias\"],\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    self.label_embed = [\n",
    "        tf.keras.layers.Dense(\n",
    "            time_embed_dim,\n",
    "            weights=[\n",
    "                weights[\"state_dict\"][tname + \".label_emb.0.0.weight\"].T,\n",
    "                weights[\"state_dict\"][tname + \".label_emb.0.0.bias\"],\n",
    "            ],\n",
    "        ),\n",
    "        tf.keras.activations.swish,\n",
    "        tf.keras.layers.Dense(\n",
    "            time_embed_dim,\n",
    "            weights=[\n",
    "                weights[\"state_dict\"][tname + \".label_emb.0.2.weight\"].T,\n",
    "                weights[\"state_dict\"][tname + \".label_emb.0.2.bias\"],\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    self.input_blocks = [\n",
    "        [\n",
    "            get_conv2d(\n",
    "                \"{}.input_blocks.0.0\".format(tname),\n",
    "                in_channels,\n",
    "                model_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            )\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    self.num_res_blocks = len(channel_mult) * [num_res_blocks]\n",
    "\n",
    "    # print(\"input\")\n",
    "    input_block_chans = [model_channels]\n",
    "    ch = model_channels\n",
    "    ds = 1\n",
    "    q = 1\n",
    "    for level, mult in enumerate(channel_mult):\n",
    "      for nr in range(self.num_res_blocks[level]):\n",
    "        # print(\"resblock {}.input_blocks.{}.0\".format(tname, q))\n",
    "        layers = [\n",
    "            [\n",
    "                ResBlock(\n",
    "                    \"{}.input_blocks.{}.0\".format(tname, q),\n",
    "                    ch,\n",
    "                    time_embed_dim,\n",
    "                    mult * model_channels,\n",
    "                )\n",
    "            ]\n",
    "        ]\n",
    "        ch = mult * model_channels\n",
    "        if ds in attention_resolutions:\n",
    "          if num_head_channels == -1:\n",
    "            dim_head = ch // num_heads\n",
    "          else:\n",
    "            num_heads = ch // num_head_channels\n",
    "            dim_head = num_head_channels\n",
    "\n",
    "          # print(\"spatial {}.input_blocks.{}.1\".format(tname, q))\n",
    "          # print(num_heads, dim_head)\n",
    "          layers[-1].append(\n",
    "              SpatialTransformer(\n",
    "                  \"{}.input_blocks.{}.1\".format(tname, q),\n",
    "                  transformer_depth[level],\n",
    "                  use_linear_in_transformer,\n",
    "                  ch,\n",
    "                  context_dim,\n",
    "                  num_heads,\n",
    "                  dim_head,\n",
    "              )\n",
    "          )\n",
    "        self.input_blocks.extend(layers)\n",
    "        q += 1\n",
    "        input_block_chans.append(ch)\n",
    "      if level != len(channel_mult) - 1:\n",
    "        out_ch = ch\n",
    "        # print(\"downsample {}.input_blocks.{}.0\".format(tname, q))\n",
    "        self.input_blocks.append(\n",
    "            [\n",
    "                Downsample(\n",
    "                    \"{}.input_blocks.{}.0\".format(tname, q), ch),\n",
    "            ]\n",
    "        )\n",
    "        q += 1\n",
    "        ch = out_ch\n",
    "        input_block_chans.append(ch)\n",
    "        ds *= 2\n",
    "\n",
    "    if num_head_channels == -1:\n",
    "      dim_head = ch // num_heads\n",
    "    else:\n",
    "      num_heads = ch // num_head_channels\n",
    "      dim_head = num_head_channels\n",
    "\n",
    "    self.middle_block = [\n",
    "        ResBlock(\"{}.middle_block.0\".format(\n",
    "            tname), ch, time_embed_dim, ch),\n",
    "        SpatialTransformer(\n",
    "            \"{}.middle_block.1\".format(tname),\n",
    "            transformer_depth[-1],\n",
    "            use_linear_in_transformer,\n",
    "            ch,\n",
    "            context_dim,\n",
    "            num_heads,\n",
    "            dim_head,\n",
    "        ),\n",
    "        ResBlock(\"{}.middle_block.2\".format(\n",
    "            tname), ch, time_embed_dim, ch),\n",
    "    ]\n",
    "\n",
    "    q = 0\n",
    "    # print(\"output\")\n",
    "    self.output_blocks = []\n",
    "    for level, mult in list(enumerate(channel_mult))[::-1]:\n",
    "      for i in range(self.num_res_blocks[level] + 1):\n",
    "        blub = False\n",
    "        ich = input_block_chans.pop()\n",
    "        # print(\"resblock {}.output_blocks.{}.0\".format(tname, q))\n",
    "        layers = [\n",
    "            [\n",
    "                ResBlock(\n",
    "                    \"{}.output_blocks.{}.0\".format(tname, q),\n",
    "                    ch + ich,\n",
    "                    time_embed_dim,\n",
    "                    model_channels * mult,\n",
    "                )\n",
    "            ]\n",
    "        ]\n",
    "        ch = model_channels * mult\n",
    "        if ds in attention_resolutions:\n",
    "          if num_head_channels == -1:\n",
    "            dim_head = ch // num_heads\n",
    "          else:\n",
    "            num_heads = ch // num_head_channels\n",
    "            dim_head = num_head_channels\n",
    "\n",
    "          blub = True\n",
    "          # print(\"spatial {}.output_blocks.{}.1\".format(tname, q))\n",
    "          # print(num_heads, dim_head)\n",
    "          layers[-1].append(\n",
    "              SpatialTransformer(\n",
    "                  \"{}.output_blocks.{}.1\".format(tname, q),\n",
    "                  transformer_depth[level],\n",
    "                  use_linear_in_transformer,\n",
    "                  ch,\n",
    "                  context_dim,\n",
    "                  num_heads,\n",
    "                  dim_head,\n",
    "              )\n",
    "          )\n",
    "        if level and i == self.num_res_blocks[level]:\n",
    "          out_ch = ch\n",
    "          # print(\"upsample {}.output_blocks.{}.{}\".format(tname, q, 2 if blub else 1))\n",
    "          layers[-1].append(\n",
    "              Upsample(\n",
    "                  \"{}.output_blocks.{}.{}\".format(\n",
    "                      tname, q, 2 if blub else 1),\n",
    "                  ch,\n",
    "              ),\n",
    "          )\n",
    "          ds //= 2\n",
    "        q += 1\n",
    "        self.output_blocks.extend(layers)\n",
    "\n",
    "    # print(*self.input_blocks, sep='\\n')\n",
    "    # print()\n",
    "    # print(*self.middle_block, sep='\\n')\n",
    "    # print()\n",
    "    # print(*self.output_blocks, sep='\\n')\n",
    "    self.out = [\n",
    "        tf.keras.layers.GroupNormalization(\n",
    "            epsilon=1e-5,\n",
    "            weights=[\n",
    "                weights[\"state_dict\"][\"{}.out.0.weight\".format(tname)],\n",
    "                weights[\"state_dict\"][\"{}.out.0.bias\".format(tname)],\n",
    "            ],\n",
    "        ),\n",
    "        tf.keras.activations.swish,\n",
    "        get_conv2d(\n",
    "            \"{}.out.2\".format(tname),\n",
    "            model_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "  def __call__(self, inputs, training=False):\n",
    "    # TODO: real time embedding\n",
    "    [x, t_emb, context, context_channel] = inputs\n",
    "    t_emb_q = apply_seq(t_emb, self.time_embed)\n",
    "    label_emb = apply_seq(context_channel, self.label_embed)\n",
    "    emb = t_emb_q + label_emb\n",
    "\n",
    "    def run(x, bb):\n",
    "      print(bb)        \n",
    "      if isinstance(bb, ResBlock):\n",
    "        x = bb([x, emb])\n",
    "      elif isinstance(bb, SpatialTransformer):\n",
    "        x = bb([x, context])\n",
    "      else:\n",
    "        x = bb(x)\n",
    "      return x\n",
    "\n",
    "    print(emb)\n",
    "    \n",
    "    saved_inputs = []\n",
    "    for i, b in enumerate(self.input_blocks):\n",
    "      for bb in b:\n",
    "        inppp = x\n",
    "        x = run(x, bb)\n",
    "      saved_inputs.append(x)\n",
    "\n",
    "    print(self.input_blocks)\n",
    "    print(self.middle_block)\n",
    "    print(self.output_blocks)\n",
    "      \n",
    "    for bb in self.middle_block:\n",
    "      x = run(x, bb)\n",
    "\n",
    "    for i, b in enumerate(self.output_blocks):\n",
    "      x = tf.concat([x, saved_inputs.pop()], axis=-1)\n",
    "      for bb in b:\n",
    "        x = run(x, bb)\n",
    "    c = apply_seq(x, self.out)\n",
    "    print(c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d976abba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "text_max_len = 77\n",
    "\n",
    "\n",
    "def get_text_enc():\n",
    "  input_word_ids = tf.keras.layers.Input(\n",
    "      shape=(text_max_len,), dtype=tf.int32)\n",
    "  input_pos_ids = tf.keras.layers.Input(\n",
    "      shape=(text_max_len,), dtype=tf.int32)\n",
    "  embeds_1 = CLIPTextTransformer(\"conditioner.embedders.0.transformer.text_model\")(\n",
    "      [input_word_ids, input_pos_ids]\n",
    "  )\n",
    "\n",
    "  # TODO: Eventually migrate the Pytorch open_clip to here instead\n",
    "  # embeds_2 = CLIPTextTransformer('conditioner.embedders.1.model', True)([input_word_ids, input_pos_ids])\n",
    "\n",
    "  # return tf.keras.models.Model([input_word_ids,input_pos_ids], tf.concat([embeds_1, embeds_2], 2))\n",
    "  return tf.keras.models.Model([input_word_ids, input_pos_ids], embeds_1)\n",
    "\n",
    "\n",
    "def get_unet():\n",
    "  t_emb = tf.keras.layers.Input((320,))\n",
    "  latent = tf.keras.layers.Input((128, 128, 4))\n",
    "  context = tf.keras.layers.Input((text_max_len, 2048))\n",
    "  context_channel = tf.keras.layers.Input((2816,))\n",
    "\n",
    "  # in_channels,\n",
    "  # out_channels,\n",
    "  # model_channels,\n",
    "  # attention_resolutions,\n",
    "  # num_res_blocks,\n",
    "  # channel_mult,\n",
    "  # num_heads\n",
    "  # num_head_channels\n",
    "  # transformer_depth\n",
    "  # context_dim,\n",
    "  unet = UNetModel(\n",
    "      \"model.diffusion_model\",\n",
    "      4,\n",
    "      4,\n",
    "      320,\n",
    "      [4, 2],\n",
    "      2,\n",
    "      [1, 2, 4],\n",
    "      -1,\n",
    "      64,\n",
    "      True,\n",
    "      [1, 2, 10],\n",
    "      2048,\n",
    "  )\n",
    "  # unet = UNetModel(\"model.diffusion_model\", 4, 4, 320, [4, 2, 1], 2, [1, 2, 4, 4], 8, -1, 768)\n",
    "\n",
    "  return tf.keras.models.Model(\n",
    "      [latent, t_emb, context, context_channel], \n",
    "      unet([latent, t_emb, context, context_channel])\n",
    "  )\n",
    "\n",
    "def get_decoder():\n",
    "  latent = tf.keras.layers.Input((128, 128, 4))\n",
    "  decoder = Decoder(\"first_stage_model.decoder\")\n",
    "  return tf.keras.models.Model(latent, decoder(latent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bbccaa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "2\n",
      "2\n",
      "2\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1280), dtype=tf.float32, name=None), name='tf.__operators__.add_14/AddV2:0', description=\"created by layer 'tf.__operators__.add_14'\")\n",
      "<keras.src.engine.sequential.Sequential object at 0x000001142053DA30>\n",
      "<__main__.ResBlock object at 0x00000114205A9D60>\n",
      "<__main__.ResBlock object at 0x0000011420598250>\n",
      "<__main__.Downsample object at 0x00000114205B5100>\n",
      "<__main__.ResBlock object at 0x00000114205B7070>\n",
      "<__main__.SpatialTransformer object at 0x00000114205BDCA0>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer \"multi_head_attention_210\" with a weight list of length 3, but the layer was expecting 0 weights. Provided weights: [tensor([[ 0.0219,  0.0409, -0.0544,  ...,  0.0122...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#text_encoder = get_text_enc()\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m unet \u001b[38;5;241m=\u001b[39m \u001b[43mget_unet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#decoder = get_decoder()\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[48], line 54\u001b[0m, in \u001b[0;36mget_unet\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m unet \u001b[38;5;241m=\u001b[39m UNetModel(\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.diffusion_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;241m2048\u001b[39m,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# unet = UNetModel(\"model.diffusion_model\", 4, 4, 320, [4, 2, 1], 2, [1, 2, 4, 4], 8, -1, 768)\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mModel(\n\u001b[0;32m     53\u001b[0m     [latent, t_emb, context, context_channel], \n\u001b[1;32m---> 54\u001b[0m     \u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_channel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m )\n",
      "Cell \u001b[1;32mIn[47], line 247\u001b[0m, in \u001b[0;36mUNetModel.__call__\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m    245\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m bb \u001b[38;5;129;01min\u001b[39;00m b:\n\u001b[0;32m    246\u001b[0m     inppp \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m--> 247\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m   saved_inputs\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_blocks)\n",
      "Cell \u001b[1;32mIn[47], line 236\u001b[0m, in \u001b[0;36mUNetModel.__call__.<locals>.run\u001b[1;34m(x, bb)\u001b[0m\n\u001b[0;32m    234\u001b[0m   x \u001b[38;5;241m=\u001b[39m bb([x, emb])\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(bb, SpatialTransformer):\n\u001b[1;32m--> 236\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[43mbb\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m   x \u001b[38;5;241m=\u001b[39m bb(x)\n",
      "Cell \u001b[1;32mIn[56], line 60\u001b[0m, in \u001b[0;36mSpatialTransformer.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     58\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_in(x)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks:\n\u001b[1;32m---> 60\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_linear:\n\u001b[0;32m     63\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(x)\n",
      "Cell \u001b[1;32mIn[60], line 48\u001b[0m, in \u001b[0;36mBasicTransformerBlock.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m     46\u001b[0m   [x, context] \u001b[38;5;241m=\u001b[39m inputs\n\u001b[1;32m---> 48\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m     49\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn2([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x), context]) \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m     50\u001b[0m   x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x)) \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\site-packages\\keras\\src\\engine\\base_layer.py:1797\u001b[0m, in \u001b[0;36mLayer.set_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m   1794\u001b[0m         expected_num_weights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_num_weights \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(weights):\n\u001b[1;32m-> 1797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1798\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou called `set_weights(weights)` on layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1799\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith a weight list of length \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, but the layer was \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1800\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m weights. Provided weights: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1801\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m   1802\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m   1803\u001b[0m             \u001b[38;5;28mlen\u001b[39m(weights),\n\u001b[0;32m   1804\u001b[0m             expected_num_weights,\n\u001b[0;32m   1805\u001b[0m             \u001b[38;5;28mstr\u001b[39m(weights)[:\u001b[38;5;241m50\u001b[39m],\n\u001b[0;32m   1806\u001b[0m         )\n\u001b[0;32m   1807\u001b[0m     )\n\u001b[0;32m   1809\u001b[0m weight_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1810\u001b[0m weight_value_tuples \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: You called `set_weights(weights)` on layer \"multi_head_attention_210\" with a weight list of length 3, but the layer was expecting 0 weights. Provided weights: [tensor([[ 0.0219,  0.0409, -0.0544,  ...,  0.0122..."
     ]
    }
   ],
   "source": [
    "#text_encoder = get_text_enc()\n",
    "unet = get_unet()\n",
    "#decoder = get_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45156971-22c8-4b87-96e8-bc3511d750df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered = [x for x in weights[\"state_dict\"].keys() if \"label_embed\" in x]\n",
    "print(*filtered, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43f6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet.save_weights(\"/tmp/unet.h5\")\n",
    "# decoder.save_weights(\"/tmp/decoder.h5\")\n",
    "# text_encoder.save_weights(\"/tmp/text_encoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40e7dfb-2300-4b3b-9c77-75579eaa0d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"an astronaut riding a horse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e19191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "model = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f8b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt], padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d519a0cb-abfb-41fe-b2b7-deef1dcb9682",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_openclip, _, _ = open_clip.create_model_and_transforms(\n",
    "    \"ViT-bigG-14\", pretrained=\"laion2b_s39b_b160k\"\n",
    ")\n",
    "tokenizer_openclip = open_clip.get_tokenizer(\"ViT-bigG-14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167db20-16ef-456c-9db5-410bde5dc771",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_openclip = tokenizer_openclip([prompt]).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7171475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed88ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_ids = tf.convert_to_tensor(np.array(list(range(77)))[None].astype('int32'))\n",
    "# pos_ids = np.repeat(pos_ids, batch_size, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c7c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = inputs[\"input_ids\"][0] + [49407] * (77 - len(inputs[\"input_ids\"][0]))\n",
    "\n",
    "phrase = np.array(phrase)[None].astype(\"int32\")\n",
    "phrase = np.repeat(phrase, batch_size, axis=0)\n",
    "# phrase = tf.convert_to_tensor(phrase)\n",
    "# context_clip = text_encoder([phrase])\n",
    "# context_clip.shape\n",
    "\n",
    "phrase = torch.from_numpy(phrase)\n",
    "context_clip = model(phrase)\n",
    "context_clip = context_clip.last_hidden_state.detach().numpy()\n",
    "context_clip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a2f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = [49406] * 77\n",
    "phrase = np.array(phrase)[None].astype(\"int32\")\n",
    "phrase = np.repeat(phrase, batch_size, axis=0)\n",
    "\n",
    "# phrase = tf.convert_to_tensor(phrase)\n",
    "# unconditional_context_clip = text_encoder([phrase])\n",
    "# unconditional_context_clip.shape\n",
    "\n",
    "phrase = torch.from_numpy(phrase)\n",
    "unconditional_context_clip = model(phrase)\n",
    "unconditional_context_clip = (\n",
    "    unconditional_context_clip.last_hidden_state.detach().numpy()\n",
    ")\n",
    "unconditional_context_clip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e59d7-75bc-4204-88db-852ef8c8f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = torch.from_numpy(inputs_openclip)\n",
    "x = model_openclip.token_embedding(phrase)\n",
    "x = x + model_openclip.positional_embedding\n",
    "x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "\n",
    "outputs = {}\n",
    "for i, r in enumerate(model_openclip.transformer.resblocks):\n",
    "  if i == len(model_openclip.transformer.resblocks) - 1:\n",
    "    outputs[\"penultimate\"] = x.permute(1, 0, 2)  # LND -> NLD\n",
    "  x = r(x, attn_mask=None)\n",
    "outputs[\"last\"] = x.permute(1, 0, 2)  # LND -> NLD\n",
    "x = outputs\n",
    "\n",
    "# x is a dict and will stay a dict\n",
    "o = x[\"last\"]\n",
    "o = model_openclip.ln_final(o)\n",
    "pooled = (\n",
    "    o[torch.arange(o.shape[0]), phrase.argmax(dim=-1)\n",
    "      ] @ model_openclip.text_projection\n",
    ")\n",
    "x[\"pooled\"] = pooled\n",
    "\n",
    "context_openclip = x[\"penultimate\"].detach().numpy()\n",
    "context_openclip_pooled = x[\"pooled\"].detach().numpy()\n",
    "\n",
    "context_openclip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8593a22d-a509-4389-9d89-5a4144c40693",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = tokenizer_openclip([\"\"]).detach().numpy()\n",
    "phrase = torch.from_numpy(aaa)\n",
    "x = model_openclip.token_embedding(phrase)\n",
    "x = x + model_openclip.positional_embedding\n",
    "x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "\n",
    "outputs = {}\n",
    "for i, r in enumerate(model_openclip.transformer.resblocks):\n",
    "  if i == len(model_openclip.transformer.resblocks) - 1:\n",
    "    outputs[\"penultimate\"] = x.permute(1, 0, 2)  # LND -> NLD\n",
    "  x = r(x, attn_mask=None)\n",
    "outputs[\"last\"] = x.permute(1, 0, 2)  # LND -> NLD\n",
    "x = outputs\n",
    "\n",
    "# x is a dict and will stay a dict\n",
    "o = x[\"last\"]\n",
    "o = model_openclip.ln_final(o)\n",
    "pooled = (\n",
    "    o[torch.arange(o.shape[0]), phrase.argmax(dim=-1)\n",
    "      ] @ model_openclip.text_projection\n",
    ")\n",
    "x[\"pooled\"] = pooled\n",
    "\n",
    "unconditional_context_openclip = x[\"penultimate\"].detach().numpy()\n",
    "unconditional_context_openclip_pooled = x[\"pooled\"].detach().numpy()\n",
    "\n",
    "unconditional_context_openclip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a5b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_embedding(timesteps, dim=320, max_period=10000):\n",
    "  half = dim // 2\n",
    "  freqs = np.exp(-math.log(max_period) *\n",
    "                 np.arange(0, half, dtype=np.float32) / half)\n",
    "  print(freqs.shape)\n",
    "  args = np.array(timesteps) * freqs\n",
    "  embedding = np.concatenate([np.cos(args), np.sin(args)])\n",
    "  return (embedding).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c1407-fa24-415f-a2e5-f5be5bcfa281",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_full = tf.concat([context_clip, context_openclip], 2)\n",
    "unconditional_context_full = tf.concat(\n",
    "    [unconditional_context_clip, unconditional_context_openclip], 2\n",
    ")\n",
    "\n",
    "def conditioning_embedding(pooled_text_enc, dim, size, crop, ar):\n",
    "    cat = tf.concat([size, crop, ar], axis=1)\n",
    "    n_batch, w = cat.shape\n",
    "    \n",
    "    # Reshape cat to [n_batch * w]\n",
    "    cat_reshaped = tf.reshape(cat, [n_batch * w])\n",
    "    \n",
    "    # Apply timestep_embedding and reshape the result\n",
    "    embed = timestep_embedding(cat_reshaped, dim, 10000)\n",
    "    embed_reshaped = tf.reshape(embed, [n_batch, w * dim])\n",
    "    \n",
    "    # Concatenate pooled_text_enc and embed along the second axis\n",
    "    return tf.concat([pooled_text_enc, embed_reshaped], axis=1)\n",
    "\n",
    "channel_context = conditioning_embedding(\n",
    "  context_openclip_pooled,\n",
    "  256,\n",
    "  [1024,1024],\n",
    "  [0,0],\n",
    "  [1024,1024],\n",
    ")\n",
    "unconditional_context_channel = conditioning_embedding(\n",
    "  unconditional_context_openclip_pooled,\n",
    "  256,\n",
    "  [1024,1024],\n",
    "  [0,0],\n",
    "  [1024,1024],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2be335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_output(latent, t):\n",
    "  # put into diffuser\n",
    "  timesteps = np.array([t])\n",
    "  t_emb = tf.convert_to_tensor(timestep_embedding(timesteps))\n",
    "  t_emb = np.repeat(t_emb, batch_size, axis=0)\n",
    "    \n",
    "  conditional_latent = unet([latent, t_emb, context, context_channel]) \n",
    "  unconditional_latent = unet([latent, t_emb, unconditional_context, unconditional_context_channel])\n",
    "  unconditional_guidance_scale = 7.5\n",
    "  e_t = unconditional_latent + unconditional_guidance_scale * (\n",
    "      conditional_latent - unconditional_latent\n",
    "  )\n",
    "  return e_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722af063",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TIMESTEPS = int(os.getenv(\"TIMESTEPS\", \"25\"))\n",
    "timesteps = list(np.arange(1, 1000, 1000 // TIMESTEPS))\n",
    "print(f\"running for {timesteps} timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d9816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_start = 0.00085\n",
    "linear_end = 0.0120\n",
    "num_timesteps = 1000\n",
    "betas = tf.linspace(tf.sqrt(linear_start), tf.sqrt(\n",
    "    linear_end), num_timesteps) ** 2\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = np.cumprod(alphas, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a698e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(alphas_cumprod.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c30cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [alphas_cumprod[t] for t in timesteps]\n",
    "alphas_prev = [1.0] + alphas[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681259b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_prev_and_pred_x0(x, e_t, index):\n",
    "  temperature = 1\n",
    "  a_t, a_prev = alphas[index], alphas_prev[index]\n",
    "  sigma_t = 0\n",
    "  sqrt_one_minus_at = math.sqrt(1 - a_t)\n",
    "  # print(a_t, a_prev, sigma_t, sqrt_one_minus_at)\n",
    "\n",
    "  pred_x0 = (x - sqrt_one_minus_at * e_t) / math.sqrt(a_t)\n",
    "\n",
    "  # direction pointing to x_t\n",
    "  dir_xt = math.sqrt(1.0 - a_prev - sigma_t**2) * e_t\n",
    "  noise = sigma_t * tf.random.normal(x.shape) * temperature\n",
    "\n",
    "  x_prev = math.sqrt(a_prev) * pred_x0 + dir_xt  # + noise\n",
    "  return x_prev, pred_x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9279713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = tf.random.normal((batch_size, 128, 128, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332f01cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, timestep in (t := tqdm(list(enumerate(timesteps))[::-1])):\n",
    "  t.set_description(\"%3d %3d\" % (index, timestep))\n",
    "  e_t = get_model_output(latent, timestep)\n",
    "  x_prev, pred_x0 = get_x_prev_and_pred_x0(latent, e_t, index)\n",
    "  latent = x_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f061f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = decoder(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac4713",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.clip((decoded[0].numpy() + 1) / 2, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcceeca-21ed-4e46-aba4-5bd621594a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
