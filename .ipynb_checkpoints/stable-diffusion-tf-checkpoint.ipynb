{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1190a76d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\James\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import math \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da17caae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b81e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "FILENAME = \"C:\\\\Users\\\\James\\\\stable-diffusion-webui\\\\models\\\\Stable-diffusion\\\\sd_xl_base_1.0.ckpt\"\n",
    "#FILENAME = \"C:\\\\Users\\\\James\\\\stable-diffusion-webui\\\\models\\\\Stable-diffusion\\\\sd_xl_base_1.0.safetensors\"\n",
    "#FILENAME = \"C:\\\\Users\\\\James\\\\Downloads\\\\v1-5-pruned.ckpt\"\n",
    "weights = torch.load(FILENAME)\n",
    "#weights = safe_open(FILENAME, framework=\"pt\", device=\"cpu\")\n",
    "#weights_f = safe_open(FILENAME, framework=\"pt\", device=\"cpu\")\n",
    "weights = {'state_dict':weights}\n",
    "#for key in weights_f.keys():\n",
    "#    weights['state_dict'][key] = weights_f.get_tensor(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f92611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_gelu(x):\n",
    "    return x * tf.sigmoid(x * 1.702) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce7eb2b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    " class CLIPAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, tname):\n",
    "        super(CLIPAttention, self).__init__()\n",
    "        self.embed_dim = 768\n",
    "        self.num_heads = 12\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        self.k_proj = tf.keras.layers.Dense(self.embed_dim, activation=None, weights=[weights['state_dict'][tname+\".k_proj.weight\"].T, weights['state_dict'][tname+\".k_proj.bias\"]] )\n",
    "        self.v_proj = tf.keras.layers.Dense(self.embed_dim, activation=None, weights=[weights['state_dict'][tname+\".v_proj.weight\"].T, weights['state_dict'][tname+\".v_proj.bias\"]])\n",
    "        self.q_proj = tf.keras.layers.Dense(self.embed_dim, activation=None, weights=[weights['state_dict'][tname+\".q_proj.weight\"].T, weights['state_dict'][tname+\".q_proj.bias\"]])\n",
    "        self.out_proj = tf.keras.layers.Dense(self.embed_dim, activation=None, weights=[weights['state_dict'][tname+\".out_proj.weight\"].T, weights['state_dict'][tname+\".out_proj.bias\"]])\n",
    "\n",
    "    def _shape(self, tensor, seq_len: int, bsz: int):\n",
    "        a = tf.reshape(tensor , (bsz, seq_len, self.num_heads, self.head_dim))\n",
    "        return tf.keras.layers.Permute( (2,1,3) )(a) # bs , n_head , seq_len , head_dim\n",
    "    \n",
    "    def call(self, inputs ):\n",
    "        [hidden_states, causal_attention_mask] = inputs\n",
    "        bsz, tgt_len, embed_dim = hidden_states.shape\n",
    "        query_states = self.q_proj(hidden_states) * self.scale\n",
    "        key_states = self._shape(self.k_proj(hidden_states), tgt_len, -1)\n",
    "        value_states = self._shape(self.v_proj(hidden_states), tgt_len, -1)\n",
    "        \n",
    "        proj_shape = (-1 , tgt_len, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, -1)\n",
    "        query_states = tf.reshape(query_states , proj_shape)\n",
    "        key_states = tf.reshape(key_states , proj_shape)\n",
    "                \n",
    "        src_len = tgt_len\n",
    "        value_states = tf.reshape(value_states , proj_shape)\n",
    "        attn_weights = query_states @ tf.keras.layers.Permute((2,1))(key_states) \n",
    "        \n",
    "        attn_weights = tf.reshape(attn_weights , (-1, self.num_heads, tgt_len, src_len))\n",
    "        attn_weights = attn_weights + causal_attention_mask\n",
    "        attn_weights = tf.reshape(attn_weights , (-1, tgt_len, src_len))\n",
    "        \n",
    "        attn_weights = tf.nn.softmax(attn_weights)\n",
    "        attn_output = attn_weights @ value_states\n",
    "        \n",
    "        attn_output = tf.reshape(attn_output , (-1, self.num_heads, tgt_len, self.head_dim))\n",
    "        attn_output = tf.keras.layers.Permute((2,1,3))(attn_output)\n",
    "        attn_output = tf.reshape(attn_output , (-1, tgt_len, embed_dim))\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        return attn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0959b341-3bbe-48af-b03d-d67be1e56c86",
   "metadata": {},
   "outputs": [],
   "source": [
    " class OpenCLIPAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, tname):\n",
    "        super(OpenCLIPAttention, self).__init__()    \n",
    "        self.embed_dim = 1280\n",
    "        self.num_heads = 12\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.scale = self.head_dim**-0.5\n",
    "        \n",
    "        in_proj_weight = weights['state_dict'][tname+\".in_proj_weight\"]\n",
    "        in_proj_bias = weights['state_dict'][tname+\".in_proj_bias\"]\n",
    "\n",
    "        q_weight, k_weight, v_weight = tf.split(in_proj_weight, [self.embed_dim, self.embed_dim, self.embed_dim])\n",
    "        q_bias, k_bias, v_bias = tf.split(in_proj_bias, [self.embed_dim, self.embed_dim, self.embed_dim])\n",
    "\n",
    "        self.k_proj = tf.keras.layers.Dense(self.embed_dim, activation=None, weights=[k_weight, k_bias])\n",
    "        self.v_proj = tf.keras.layers.Dense(self.embed_dim, activation=None, weights=[v_weight, q_bias])\n",
    "        self.q_proj = tf.keras.layers.Dense(self.embed_dim, activation=None, weights=[q_weight, q_bias])\n",
    "        self.out_proj = tf.keras.layers.Dense(self.embed_dim, activation=None, weights=[weights['state_dict'][tname+\".out_proj.weight\"].T, weights['state_dict'][tname+\".out_proj.bias\"]])\n",
    "\n",
    "    def _shape(self, tensor, seq_len: int, bsz: int):\n",
    "        a = tf.reshape(tensor , (bsz, seq_len, self.num_heads, self.head_dim))\n",
    "        return tf.keras.layers.Permute( (2,1,3) )(a) # bs , n_head , seq_len , head_dim\n",
    "    \n",
    "    def call(self, inputs ):\n",
    "        [hidden_states, causal_attention_mask] = inputs\n",
    "        bsz, tgt_len, embed_dim = hidden_states.shape\n",
    "        query_states = self.q_proj(hidden_states) * self.scale\n",
    "        key_states = self._shape(self.k_proj(hidden_states), tgt_len, -1)\n",
    "        value_states = self._shape(self.v_proj(hidden_states), tgt_len, -1)\n",
    "        \n",
    "        proj_shape = (-1 , tgt_len, self.head_dim)\n",
    "        query_states = self._shape(query_states, tgt_len, -1)\n",
    "        query_states = tf.reshape(query_states , proj_shape)\n",
    "        key_states = tf.reshape(key_states , proj_shape)\n",
    "                \n",
    "        src_len = tgt_len\n",
    "        value_states = tf.reshape(value_states , proj_shape)\n",
    "        attn_weights = query_states @ tf.keras.layers.Permute((2,1))(key_states) \n",
    "        \n",
    "        attn_weights = tf.reshape(attn_weights , (-1, self.num_heads, tgt_len, src_len))\n",
    "        attn_weights = attn_weights + causal_attention_mask\n",
    "        attn_weights = tf.reshape(attn_weights , (-1, tgt_len, src_len))\n",
    "        \n",
    "        attn_weights = tf.nn.softmax(attn_weights)\n",
    "        attn_output = attn_weights @ value_states\n",
    "        \n",
    "        attn_output = tf.reshape(attn_output , (-1, self.num_heads, tgt_len, self.head_dim))\n",
    "        attn_output = tf.keras.layers.Permute((2,1,3))(attn_output)\n",
    "        attn_output = tf.reshape(attn_output , (-1, tgt_len, embed_dim))\n",
    "\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        return attn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82bec30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPMLP(tf.keras.layers.Layer):\n",
    "    def __init__(self, tname):\n",
    "        super(CLIPMLP, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense( 3072, weights=[weights['state_dict'][tname+\".fc1.weight\"].T  , weights['state_dict'][tname+\".fc1.bias\"]])\n",
    "        self.fc2 = tf.keras.layers.Dense(  768, weights=[weights['state_dict'][tname+\".fc2.weight\"].T  , weights['state_dict'][tname+\".fc2.bias\"]])\n",
    "\n",
    "    def __call__(self, hidden_states):\n",
    "        hidden_states = self.fc1(hidden_states)\n",
    "        hidden_states = quick_gelu(hidden_states )\n",
    "        hidden_states = self.fc2(hidden_states)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cf73f23-400f-45cd-a838-752f52060ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenCLIPMLP(tf.keras.layers.Layer):\n",
    "    def __init__(self, tname):\n",
    "        super(OpenCLIPMLP, self).__init__()\n",
    "        self.c_fc = tf.keras.layers.Dense( 3072, weights=[weights['state_dict'][tname+\".c_fc.weight\"].T  , weights['state_dict'][tname+\".c_fc.bias\"]])\n",
    "        self.c_proj = tf.keras.layers.Dense(  768, weights=[weights['state_dict'][tname+\".c_proj.weight\"].T  , weights['state_dict'][tname+\".c_proj.bias\"]])\n",
    "\n",
    "    def __call__(self, hidden_states):\n",
    "        hidden_states = self.c_fc(hidden_states)\n",
    "        hidden_states = gelu(hidden_states)\n",
    "        hidden_states = self.c_proj(hidden_states)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99437cb8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CLIPEncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, tname):\n",
    "        super(CLIPEncoderLayer, self).__init__()\n",
    "        self.self_attn = CLIPAttention(tname+\".self_attn\")\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization( epsilon=1e-5 , weights=[ weights['state_dict'][tname+\".layer_norm1.weight\"] , weights['state_dict'][tname+\".layer_norm1.bias\"] ])\n",
    "        self.mlp = CLIPMLP(tname+\".mlp\")\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization( epsilon=1e-5 , weights=[ weights['state_dict'][tname+\".layer_norm2.weight\"] , weights['state_dict'][tname+\".layer_norm2.bias\"] ])\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        [hidden_states, causal_attention_mask] = inputs\n",
    "        residual = hidden_states\n",
    "    \n",
    "        hidden_states = self.layer_norm1(hidden_states)\n",
    "        hidden_states = self.self_attn([hidden_states, causal_attention_mask])\n",
    "        hidden_states = residual + hidden_states\n",
    "        \n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        \n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "292cfca2-f1a6-4074-bf2c-9f503814596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenCLIPEncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, tname):\n",
    "        super(OpenCLIPEncoderLayer, self).__init__()\n",
    "        self.self_attn = OpenCLIPAttention(tname+\".attn\")\n",
    "        self.mlp = OpenCLIPMLP(tname+\".mlp\")\n",
    "        \n",
    "        #ln_1 or attn_ln\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-5 , weights=[ weights['state_dict'][tname+\".ln_1.weight\"] , weights['state_dict'][tname+\".ln_1.bias\"] ])\n",
    "        #ln_2 or mlp_ln\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-5 , weights=[ weights['state_dict'][tname+\".ln_2.weight\"] , weights['state_dict'][tname+\".ln_2.bias\"] ])\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        [hidden_states, causal_attention_mask] = inputs\n",
    "        residual = hidden_states\n",
    "    \n",
    "        hidden_states = self.layer_norm1(hidden_states)\n",
    "        hidden_states = self.self_attn([hidden_states, causal_attention_mask])\n",
    "        hidden_states = residual + hidden_states\n",
    "        \n",
    "        residual = hidden_states\n",
    "        hidden_states = self.layer_norm2(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        \n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad2012e3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CLIPEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, tname, openclip=False):\n",
    "        super(CLIPEncoder, self).__init__()\n",
    "        if (openclip):\n",
    "            self.layers = [OpenCLIPEncoderLayer(tname + \".resblocks.%d\"%i) for i in range(12)]\n",
    "        else:\n",
    "            self.layers = [CLIPEncoderLayer(tname + \".layers.%d\"%i) for i in range(12)]\n",
    "  \n",
    "    def __call__(self, inputs):\n",
    "        [hidden_states, causal_attention_mask] = inputs\n",
    "        for i,l in enumerate(self.layers):\n",
    "            hidden_states = l([hidden_states, causal_attention_mask])\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49911587",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPTextEmbeddings(tf.keras.layers.Layer):\n",
    "    def __init__(self, tname, openclip=False, n_words=77):\n",
    "        super(CLIPTextEmbeddings, self).__init__()\n",
    "        \n",
    "        token_emb_W = weights['state_dict'][tname + '.token_embedding.weight']\n",
    "        self.token_embedding_layer = tf.keras.layers.Embedding( 49408, 1280 if openclip else 768, name=\"token_embedding\" , weights=[token_emb_W])\n",
    "\n",
    "        pos_emb_W = weights['state_dict'][tname + ('.positional_embedding' if openclip else '.position_embedding.weight')]\n",
    "        self.position_embedding_layer = tf.keras.layers.Embedding( n_words, 1280 if openclip else 768, name=\"position_embedding\", weights=[pos_emb_W])\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        [input_ids, position_ids] = inputs\n",
    "        word_embeddings = self.token_embedding_layer(input_ids)\n",
    "        position_embeddings = self.position_embedding_layer(position_ids)\n",
    "        return word_embeddings + position_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22bc7cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPTextTransformer(tf.keras.models.Model):\n",
    "    def __init__(self, tname, openclip=False, n_words=77):\n",
    "        super(CLIPTextTransformer, self).__init__()\n",
    "        #Contains both token and position embedding\n",
    "        self.embeddings = CLIPTextEmbeddings(tname+(\"\" if openclip else \".embeddings\"), openclip, n_words=n_words)\n",
    "\n",
    "        # Corresponds to blocks\n",
    "        self.encoder = CLIPEncoder(tname + (\".transformer\" if openclip else \".encoder\"), openclip)\n",
    "\n",
    "        self.final_layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-5 , weights=[ weights['state_dict'][tname+(\".ln_final.weight\" if openclip else \".final_layer_norm.weight\")] , weights['state_dict'][tname+(\".ln_final.bias\" if openclip else \".final_layer_norm.bias\")] ])\n",
    "\n",
    "        if (openclip):\n",
    "            self.text_projection = tf.keras.layers.Dense(512, weights=[weights['state_dict'][tname+\".text_projection.weight\"].T  , weights['state_dict'][tname+\".text_projection.bias\"]])\n",
    "        else:\n",
    "            self.text_projection = None\n",
    "        \n",
    "        self.causal_attention_mask = tf.constant(np.triu(np.ones((1,1,77,77), dtype=np.float32) * -np.inf, k=1))\n",
    "        \n",
    "    def __call__(self, inputs , training=False):\n",
    "        [input_ids, inp_position_ids] = inputs\n",
    "        x = self.embeddings([input_ids, inp_position_ids])\n",
    "        x = self.encoder([x, self.causal_attention_mask])\n",
    "        \n",
    "        # TODO: figure out the format of text\n",
    "        #if self.text_projection is not None:\n",
    "            #eot_indices = tf.argmax(text, axis=1)\n",
    "            #normed = self.final_layer_norm(x)\n",
    "            #o = tf.squeeze(tf.gather(normed, eot_indices, axis=1), axis=1)\n",
    "        \n",
    "            #t_proj = self.text_projection.val()\n",
    "        #    pooled = o #tf.matmul(o, t_proj)\n",
    "        #else:\n",
    "        #    pooled = o\n",
    "        \n",
    "        return self.final_layer_norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fad6082",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_conv2d(tname , in_channels, out_channels, kernel_size ,padding=0 ,stride=1):\n",
    "    W =  weights['state_dict'][tname + '.weight'].numpy()\n",
    "    if (len(W.shape) > 2):\n",
    "        W = np.transpose(W , (2,3,1,0))\n",
    "    else:\n",
    "        W = np.transpose(W)\n",
    "    assert in_channels == W.shape[-2]\n",
    "    assert out_channels == W.shape[-1]\n",
    "    b =  weights['state_dict'][tname + '.bias']\n",
    "    \n",
    "    r =  tf.keras.models.Sequential([\n",
    "        \n",
    "        tf.keras.layers.ZeroPadding2D(\n",
    "            padding=(padding, padding), data_format=None \n",
    "        ) , \n",
    "        \n",
    "        \n",
    "        tf.keras.layers.Conv2D(\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            strides=(stride, stride ) , weights=[W,b] ) \n",
    "    ])\n",
    "    return r    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e52b28f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_seq(x , layers):\n",
    "    for l in layers:\n",
    "        x = l(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af24d9fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# not to be confused with ResnetBlock\n",
    "class ResBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname, channels, emb_channels, out_channels):\n",
    "    super(ResBlock, self).__init__()\n",
    "    self.in_layers = [\n",
    "      tf.keras.layers.GroupNormalization(epsilon=1e-5 , weights=[\n",
    "        weights['state_dict'][tname+\".in_layers.0.weight\"],\n",
    "        weights['state_dict'][tname+\".in_layers.0.bias\"]\n",
    "      ]),\n",
    "      tf.keras.activations.swish,\n",
    "      get_conv2d(tname+\".in_layers.2\" , channels, out_channels, 3, padding=1)\n",
    "    ]\n",
    "    self.emb_layers = [\n",
    "      tf.keras.activations.swish,\n",
    "      tf.keras.layers.Dense( out_channels, weights=[weights['state_dict'][tname+\".emb_layers.1.weight\"].T  , weights['state_dict'][tname+\".emb_layers.1.bias\"]])\n",
    "    ]\n",
    "    self.out_layers = [\n",
    "      tf.keras.layers.GroupNormalization(epsilon=1e-5 , weights=[\n",
    "        weights['state_dict'][tname+\".out_layers.0.weight\"],\n",
    "        weights['state_dict'][tname+\".out_layers.0.bias\"]\n",
    "      ]),\n",
    "      tf.keras.activations.swish,\n",
    "      lambda x: x,\n",
    "      get_conv2d(tname+\".out_layers.3\" ,out_channels, out_channels, 3, padding=1)\n",
    "    ]\n",
    "    self.skip_connection = get_conv2d(tname+\".skip_connection\" , channels, out_channels, 1) if channels != out_channels else lambda x: x\n",
    "\n",
    "  def __call__(self,inputs ):\n",
    "    x, emb = inputs\n",
    "    h = apply_seq(x , self.in_layers)\n",
    "    emb_out = apply_seq(emb, self.emb_layers)\n",
    "    h = h + emb_out[:, None , None]\n",
    "    h = apply_seq(h , self.out_layers)\n",
    "    ret = self.skip_connection(x) + h\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ed9c0f9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def td_dot(a , b ):\n",
    "    assert len(a.shape) == 4\n",
    "    assert len(a.shape) == 4\n",
    "    assert a.shape[0] == b.shape[0]\n",
    "    assert b.shape[1] == a.shape[1]\n",
    "    aa = tf.reshape(a , (-1 , a.shape[2] , a.shape[3]))\n",
    "    bb = tf.reshape(b , (-1 , b.shape[2] , b.shape[3]))\n",
    "    cc = tf.keras.backend.batch_dot(aa , bb )\n",
    "    c = tf.reshape(cc , (-1 , a.shape[1] , cc.shape[1] , cc.shape[2]))\n",
    "    return c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22d9ffe2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname, query_dim, context_dim, n_heads, d_head):\n",
    "    super(CrossAttention, self).__init__()\n",
    "    self.to_q = tf.keras.layers.Dense( n_heads*d_head,use_bias=False ,weights=[weights['state_dict'][tname+\".to_q.weight\"].T  ])\n",
    "    self.to_k = tf.keras.layers.Dense( n_heads*d_head,use_bias=False ,weights=[weights['state_dict'][tname+\".to_k.weight\"].T  ])\n",
    "    self.to_v = tf.keras.layers.Dense( n_heads*d_head,use_bias=False ,weights=[weights['state_dict'][tname+\".to_v.weight\"].T  ])\n",
    "    self.scale = d_head ** -0.5\n",
    "    self.num_heads = n_heads\n",
    "    self.head_size = d_head\n",
    "    self.to_out = [tf.keras.layers.Dense( n_heads*d_head ,weights=[weights['state_dict'][tname+\".to_out.0.weight\"].T , weights['state_dict'][tname+\".to_out.0.bias\"] ])]\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    assert type(inputs) is list\n",
    "    if len(inputs) == 1:\n",
    "        inputs = inputs + [None]\n",
    "    [x , context] = inputs\n",
    "    context = x if context is None else context\n",
    "    q,k,v = self.to_q(x), self.to_k(context), self.to_v(context)\n",
    "    assert len(x.shape) == 3\n",
    "    q = tf.reshape(q , ( -1 , x.shape[1], self.num_heads, self.head_size) )\n",
    "    k = tf.reshape(k , ( -1 , context.shape[1] , self.num_heads, self.head_size) )\n",
    "    v = tf.reshape(v, ( -1 , context.shape[1] , self.num_heads, self.head_size) )\n",
    "    \n",
    "    q = tf.keras.layers.Permute((2,1,3))(q)  # (bs, num_heads, time, head_size)\n",
    "    k = tf.keras.layers.Permute((2,3,1))(k)  # (bs, num_heads, head_size, time)\n",
    "    v = tf.keras.layers.Permute((2,1,3))(v)# (bs, num_heads, time, head_size)\n",
    "    \n",
    "    score = td_dot(q,k) * self.scale\n",
    "    weights = tf.keras.activations.softmax(score)                   # (bs, num_heads, time, time)\n",
    "    attention = td_dot(weights,v)  \n",
    "    attention = tf.keras.layers.Permute((2,1,3))(attention) # (bs, time, num_heads, head_size)\n",
    "\n",
    "    h_ = tf.reshape(attention, (-1 ,x.shape[1] , self.num_heads * self.head_size))\n",
    "    return apply_seq(h_ , self.to_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e29258b8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def gelu(self):\n",
    "    return 0.5 * self * (1 + tf.keras.activations.tanh(self * 0.7978845608 * (1 + 0.044715 * self * self)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93f9beb0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class GEGLU(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname, dim_in, dim_out):\n",
    "    super(GEGLU, self).__init__()\n",
    "    \n",
    "    self.proj = tf.keras.layers.Dense( dim_out * 2 ,weights=[weights['state_dict'][tname+\".proj.weight\"].T , weights['state_dict'][tname+\".proj.bias\"] ])\n",
    "    self.dim_out = dim_out\n",
    "\n",
    "  def __call__(self, x):\n",
    "    xp = self.proj(x)\n",
    "    x, gate = xp[... , :self.dim_out ] , xp[..., self.dim_out:]\n",
    "    ans = x * gelu(gate)\n",
    "    \n",
    "    return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53c692a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname , dim, mult=4):\n",
    "    super(FeedForward, self).__init__()\n",
    "    self.net = [\n",
    "      GEGLU(tname + \".net.0\" , dim, dim*mult),\n",
    "      lambda x: x,\n",
    "      tf.keras.layers.Dense( dim ,weights=[weights['state_dict'][tname+\".net.2.weight\"].T , weights['state_dict'][tname+\".net.2.bias\"] ])\n",
    "    ]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return apply_seq(x, self.net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13237bd7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class BasicTransformerBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname, dim, context_dim, n_heads, d_head):\n",
    "    super(BasicTransformerBlock, self).__init__()\n",
    "    self.attn1 = CrossAttention(tname+\".attn1\" , dim, dim, n_heads, d_head)\n",
    "    self.ff = FeedForward(tname+\".ff\" ,  dim)\n",
    "    self.attn2 = CrossAttention(tname+\".attn2\" ,dim, context_dim, n_heads, d_head)\n",
    "    self.norm1 = tf.keras.layers.LayerNormalization( epsilon=1e-5 , weights=[ weights['state_dict'][tname+\".norm1.weight\"] , weights['state_dict'][tname+\".norm1.bias\"] ])\n",
    "    self.norm2 = tf.keras.layers.LayerNormalization( epsilon=1e-5 , weights=[ weights['state_dict'][tname+\".norm2.weight\"] , weights['state_dict'][tname+\".norm2.bias\"] ])\n",
    "    self.norm3 = tf.keras.layers.LayerNormalization( epsilon=1e-5 , weights=[ weights['state_dict'][tname+\".norm3.weight\"] , weights['state_dict'][tname+\".norm3.bias\"] ])\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    [x, context] = inputs\n",
    "    \n",
    "    x = self.attn1([self.norm1(x)]) + x\n",
    "    x = self.attn2([self.norm2(x), context]) + x\n",
    "    x = self.ff(self.norm3(x)) + x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b2cebc5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class SpatialTransformer(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname , channels, context_dim, n_heads, d_head):\n",
    "    super(SpatialTransformer, self).__init__()\n",
    "    self.norm = tf.keras.layers.GroupNormalization(epsilon=1e-5 , weights=[\n",
    "        weights['state_dict'][tname+\".norm.weight\"],\n",
    "        weights['state_dict'][tname+\".norm.bias\"]\n",
    "      ])\n",
    "    \n",
    "    assert channels == n_heads * d_head\n",
    "    self.proj_in = get_conv2d(tname+\".proj_in\", channels, n_heads * d_head, 1)\n",
    "    self.transformer_blocks = [BasicTransformerBlock(tname + \".transformer_blocks.0\",  channels, context_dim, n_heads, d_head)]\n",
    "    self.proj_out = get_conv2d(tname+\".proj_out\" , n_heads * d_head, channels, 1)\n",
    "\n",
    "  def __call__(self, inputs):\n",
    "    [x , context] = inputs\n",
    "    b, h, w , c  = x.shape\n",
    "    x_in = x\n",
    "    x = self.norm(x)\n",
    "    x = self.proj_in(x)\n",
    "    x = tf.reshape(x , (-1,   h*w , c) ) \n",
    "    for block in self.transformer_blocks:\n",
    "      x = block([x, context])\n",
    "    x = tf.reshape(x , (-1 , h, w , c) )\n",
    "    ret = self.proj_out(x) + x_in\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd5beb4a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Downsample(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname , channels):\n",
    "    super(Downsample, self).__init__()\n",
    "    self.op = get_conv2d(tname + \".op\" , channels, channels, 3, stride=2, padding=1)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return self.op(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7f08083",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Upsample(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname , channels):\n",
    "    super(Upsample, self).__init__()\n",
    "    self.conv = get_conv2d(tname + \".conv\" ,channels, channels, 3, padding=1)\n",
    "    self.ups = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = self.ups(x)\n",
    "    return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c2bb55f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class AttnBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname , in_channels):\n",
    "    super(AttnBlock, self).__init__()\n",
    "    self.norm = tf.keras.layers.GroupNormalization(epsilon=1e-5 , weights=[\n",
    "        weights['state_dict'][tname+\".norm.weight\"],\n",
    "        weights['state_dict'][tname+\".norm.bias\"]\n",
    "      ])\n",
    "    self.q = get_conv2d(tname+\".q\" , in_channels, in_channels, 1)\n",
    "    self.k = get_conv2d(tname+\".k\" , in_channels, in_channels, 1)\n",
    "    self.v = get_conv2d(tname+\".v\" , in_channels, in_channels, 1)\n",
    "    self.proj_out = get_conv2d(tname+\".proj_out\" ,in_channels, in_channels, 1)\n",
    "\n",
    "  # copied from AttnBlock in ldm repo\n",
    "  def __call__(self, x):\n",
    "    h_ = self.norm(x)\n",
    "    q,k,v = self.q(h_), self.k(h_), self.v(h_)\n",
    "\n",
    "    # compute attention\n",
    "    b, h,w, c = q.shape\n",
    "    q = tf.reshape(q , (-1 ,h*w , c ))# b,hw,c\n",
    "    k = tf.keras.layers.Permute((3,1,2))(k)\n",
    "    k = tf.reshape(k , (-1,c,h*w)) # b,c,hw\n",
    "    w_ = q @ k\n",
    "    w_ = w_ * (c**(-0.5))\n",
    "    w_ = tf.keras.activations.softmax (w_)\n",
    "    \n",
    "\n",
    "\n",
    "    # attend to values\n",
    "    v = tf.keras.layers.Permute((3,1,2))(v)\n",
    "    v = tf.reshape(v , (-1,c,h*w))\n",
    "    w_ = tf.keras.layers.Permute((2,1))( w_)\n",
    "    h_ = v @ w_\n",
    "    h_ = tf.keras.layers.Permute((2,1))(h_)\n",
    "    h_ = tf.reshape(h_, (-1,h,w,c))\n",
    "    return x + self.proj_out(h_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9a4b791",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ResnetBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname ,  in_channels, out_channels=None):\n",
    "    super(ResnetBlock, self).__init__()\n",
    "    self.norm1 = tf.keras.layers.GroupNormalization(epsilon=1e-5 , weights=[\n",
    "        weights['state_dict'][tname+\".norm1.weight\"],\n",
    "        weights['state_dict'][tname+\".norm1.bias\"]\n",
    "      ])\n",
    "    self.conv1 = get_conv2d(tname+\".conv1\", in_channels, out_channels, 3, padding=1)\n",
    "    self.norm2 = tf.keras.layers.GroupNormalization(epsilon=1e-5 , weights=[\n",
    "        weights['state_dict'][tname+\".norm2.weight\"],\n",
    "        weights['state_dict'][tname+\".norm2.bias\"]\n",
    "      ])\n",
    "    self.conv2 = get_conv2d(tname+\".conv2\",out_channels, out_channels, 3, padding=1)\n",
    "    self.nin_shortcut = get_conv2d(tname+\".nin_shortcut\", in_channels, out_channels, 1) if in_channels != out_channels else lambda x: x\n",
    "\n",
    "  def __call__(self, x):\n",
    "    h = self.conv1(tf.keras.activations.swish(self.norm1(x)) )\n",
    "    h = self.conv2(tf.keras.activations.swish(self.norm2(h)) )\n",
    "    return self.nin_shortcut(x) + h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdd1bf15",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Mid(tf.keras.layers.Layer):\n",
    "  def __init__(self, tname , block_in):\n",
    "    super(Mid, self).__init__()\n",
    "    self.block_1 = ResnetBlock(tname+\".block_1\" , block_in, block_in)\n",
    "    self.attn_1 = AttnBlock(tname+\".attn_1\" ,block_in)\n",
    "    self.block_2 = ResnetBlock(tname+\".block_2\" , block_in, block_in)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return apply_seq(x , [self.block_1, self.attn_1, self.block_2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc3e0855",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.models.Model):\n",
    "  def __init__(self, tname ):\n",
    "    super(Decoder, self).__init__()\n",
    "    sz = [(128, 256), (256, 512), (512, 512), (512, 512)]\n",
    "    self.conv_in = get_conv2d( tname+\".conv_in\" ,  4,512,3, padding=1)\n",
    "    self.mid = Mid(tname+\".mid\" , 512)\n",
    "    self.upp = tf.keras.layers.UpSampling2D(size=(2, 2))\n",
    "    \n",
    "    self.post_quant_conv = get_conv2d(\"first_stage_model.post_quant_conv\" , 4, 4, 1)\n",
    "\n",
    "    arr = []\n",
    "    for i,s in enumerate(sz):\n",
    "      arr.append({\"block\":\n",
    "        [ResnetBlock( tname + \".up.%d.block.0\"%i , s[1], s[0]),\n",
    "         ResnetBlock( tname + \".up.%d.block.1\"%i , s[0], s[0]),\n",
    "         ResnetBlock( tname + \".up.%d.block.2\"%i , s[0], s[0])]})\n",
    "      if i != 0: arr[-1]['upsample'] = {\"conv\": get_conv2d( tname + \".up.%d.upsample.conv\"%i , s[0], s[0], 3, padding=1)}\n",
    "    self.up = arr\n",
    "\n",
    "    self.norm_out = tf.keras.layers.GroupNormalization(epsilon=1e-5 , weights=[\n",
    "        weights['state_dict'][tname+\".norm_out.weight\"],\n",
    "        weights['state_dict'][tname+\".norm_out.bias\"]\n",
    "      ])\n",
    "    self.conv_out = get_conv2d(tname+ \".conv_out\" , 128, 3, 3, padding=1)\n",
    "\n",
    "  def __call__(self, x, training=False):\n",
    "    \n",
    "    x = self.post_quant_conv(1/0.18215 * x)\n",
    "\n",
    "    x = self.conv_in(x)\n",
    "    x = self.mid(x)\n",
    "\n",
    "    for l in self.up[::-1]:\n",
    "      for b in l['block']: x = b(x)\n",
    "      if 'upsample' in l:\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.functional.interpolate.html ?\n",
    "        bs,c,py,px = x.shape\n",
    "        x = self.upp(x)\n",
    "        x = l['upsample']['conv'](x)\n",
    "\n",
    "    return self.conv_out(tf.keras.activations.swish (self.norm_out(x)) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8717414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetModel(tf.keras.models.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tname,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        model_channels,\n",
    "        attention_resolutions,\n",
    "        num_res_blocks,\n",
    "        channel_mult,\n",
    "        num_heads,\n",
    "        num_head_channels,\n",
    "        context_dim,\n",
    "    ):\n",
    "        super(UNetModel, self).__init__()\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embed = [\n",
    "            tf.keras.layers.Dense(\n",
    "                1280,\n",
    "                weights=[\n",
    "                    weights[\"state_dict\"][tname + \".time_embed.0.weight\"].T,\n",
    "                    weights[\"state_dict\"][tname + \".time_embed.0.bias\"],\n",
    "                ],\n",
    "            ),\n",
    "            tf.keras.activations.swish,\n",
    "            tf.keras.layers.Dense(\n",
    "                1280,\n",
    "                weights=[\n",
    "                    weights[\"state_dict\"][tname + \".time_embed.2.weight\"].T,\n",
    "                    weights[\"state_dict\"][tname + \".time_embed.2.bias\"],\n",
    "                ],\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        self.input_blocks = [\n",
    "            [\n",
    "                get_conv2d(\n",
    "                    \"{}.input_blocks.0.0\".format(tname),\n",
    "                    in_channels,\n",
    "                    model_channels,\n",
    "                    kernel_size=3,\n",
    "                    padding=1,\n",
    "                )\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        self.num_res_blocks = len(channel_mult) * [num_res_blocks]\n",
    "\n",
    "        # print(\"input\")\n",
    "        input_block_chans = [model_channels]\n",
    "        ch = model_channels\n",
    "        ds = 1\n",
    "        q = 1\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for nr in range(self.num_res_blocks[level]):\n",
    "                # print(\"resblock {}.input_blocks.{}.0\".format(tname, q))\n",
    "                layers = [\n",
    "                    [\n",
    "                        ResBlock(\n",
    "                            \"{}.input_blocks.{}.0\".format(tname, q),\n",
    "                            ch,\n",
    "                            time_embed_dim,\n",
    "                            mult * model_channels,\n",
    "                        )\n",
    "                    ]\n",
    "                ]\n",
    "                ch = mult * model_channels\n",
    "                if ds in attention_resolutions:\n",
    "                    if num_head_channels == -1:\n",
    "                        dim_head = ch // num_heads\n",
    "                    else:\n",
    "                        num_heads = ch // num_head_channels\n",
    "                        dim_head = num_head_channels\n",
    "\n",
    "                    # print(\"spatial {}.input_blocks.{}.1\".format(tname, q))\n",
    "                    # print(num_heads, dim_head)\n",
    "                    layers[-1].append(\n",
    "                        SpatialTransformer(\n",
    "                            \"{}.input_blocks.{}.1\".format(tname, q),\n",
    "                            ch,\n",
    "                            context_dim,\n",
    "                            num_heads,\n",
    "                            dim_head,\n",
    "                        )\n",
    "                    )\n",
    "                self.input_blocks.extend(layers)\n",
    "                q += 1\n",
    "                input_block_chans.append(ch)\n",
    "            if level != len(channel_mult) - 1:\n",
    "                out_ch = ch\n",
    "                # print(\"downsample {}.input_blocks.{}.0\".format(tname, q))\n",
    "                self.input_blocks.append(\n",
    "                    [\n",
    "                        Downsample(\"{}.input_blocks.{}.0\".format(tname, q), ch),\n",
    "                    ]\n",
    "                )\n",
    "                q += 1\n",
    "                ch = out_ch\n",
    "                input_block_chans.append(ch)\n",
    "                ds *= 2\n",
    "\n",
    "        if num_head_channels == -1:\n",
    "            dim_head = ch // num_heads\n",
    "        else:\n",
    "            num_heads = ch // num_head_channels\n",
    "            dim_head = num_head_channels\n",
    "\n",
    "        self.middle_block = [\n",
    "            ResBlock(\"{}.middle_block.0\".format(tname), ch, time_embed_dim, ch),\n",
    "            SpatialTransformer(\n",
    "                \"{}.middle_block.1\".format(tname), ch, context_dim, num_heads, dim_head\n",
    "            ),\n",
    "            ResBlock(\"{}.middle_block.2\".format(tname), ch, time_embed_dim, ch),\n",
    "        ]\n",
    "\n",
    "        q = 0\n",
    "        # print(\"output\")\n",
    "        self.output_blocks = []\n",
    "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
    "            for i in range(self.num_res_blocks[level] + 1):\n",
    "                blub = False\n",
    "                ich = input_block_chans.pop()\n",
    "                # print(\"resblock {}.output_blocks.{}.0\".format(tname, q))\n",
    "                layers = [\n",
    "                    [\n",
    "                        ResBlock(\n",
    "                            \"{}.output_blocks.{}.0\".format(tname, q),\n",
    "                            ch + ich,\n",
    "                            time_embed_dim,\n",
    "                            model_channels * mult,\n",
    "                        )\n",
    "                    ]\n",
    "                ]\n",
    "                ch = model_channels * mult\n",
    "                if ds in attention_resolutions:\n",
    "                    if num_head_channels == -1:\n",
    "                        dim_head = ch // num_heads\n",
    "                    else:\n",
    "                        num_heads = ch // num_head_channels\n",
    "                        dim_head = num_head_channels\n",
    "\n",
    "                    blub = True\n",
    "                    # print(\"spatial {}.output_blocks.{}.1\".format(tname, q))\n",
    "                    # print(num_heads, dim_head)\n",
    "                    layers[-1].append(\n",
    "                        SpatialTransformer(\n",
    "                            \"{}.output_blocks.{}.1\".format(tname, q),\n",
    "                            ch,\n",
    "                            context_dim,\n",
    "                            num_heads,\n",
    "                            dim_head,\n",
    "                        )\n",
    "                    )\n",
    "                if level and i == self.num_res_blocks[level]:\n",
    "                    out_ch = ch\n",
    "                    # print(\"upsample {}.output_blocks.{}.{}\".format(tname, q, 2 if blub else 1))\n",
    "                    layers[-1].append(\n",
    "                        Upsample(\n",
    "                            \"{}.output_blocks.{}.{}\".format(tname, q, 2 if blub else 1),\n",
    "                            ch,\n",
    "                        ),\n",
    "                    )\n",
    "                    ds //= 2\n",
    "                q += 1\n",
    "                self.output_blocks.extend(layers)\n",
    "\n",
    "        # print(*self.input_blocks, sep='\\n')\n",
    "        # print()\n",
    "        # print(*self.middle_block, sep='\\n')\n",
    "        # print()\n",
    "        # print(*self.output_blocks, sep='\\n')\n",
    "        self.out = [\n",
    "            tf.keras.layers.GroupNormalization(\n",
    "                epsilon=1e-5,\n",
    "                weights=[\n",
    "                    weights[\"state_dict\"][\"{}.out.0.weight\".format(tname)],\n",
    "                    weights[\"state_dict\"][\"{}.out.0.bias\".format(tname)],\n",
    "                ],\n",
    "            ),\n",
    "            tf.keras.activations.swish,\n",
    "            get_conv2d(\n",
    "                \"{}.out.2\".format(tname),\n",
    "                model_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def __call__(self, inputs, training=False):\n",
    "        # TODO: real time embedding\n",
    "        [x, t_emb, context] = inputs\n",
    "        emb = apply_seq(t_emb, self.time_embed)\n",
    "\n",
    "        def run(x, bb):\n",
    "            if isinstance(bb, ResBlock):\n",
    "                x = bb([x, emb])\n",
    "            elif isinstance(bb, SpatialTransformer):\n",
    "                x = bb([x, context])\n",
    "            else:\n",
    "                x = bb(x)\n",
    "            return x\n",
    "\n",
    "        saved_inputs = []\n",
    "        for i, b in enumerate(self.input_blocks):\n",
    "            for bb in b:\n",
    "                inppp = x\n",
    "                x = run(x, bb)\n",
    "            saved_inputs.append(x)\n",
    "\n",
    "        for bb in self.middle_block:\n",
    "            x = run(x, bb)\n",
    "\n",
    "        for i, b in enumerate(self.output_blocks):\n",
    "            x = tf.concat([x, saved_inputs.pop()], axis=-1)\n",
    "            for bb in b:\n",
    "                x = run(x, bb)\n",
    "        return apply_seq(x, self.out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d976abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 77\n",
    "\n",
    "def get_text_enc():\n",
    "    input_word_ids = tf.keras.layers.Input(shape=(text_max_len,), dtype=tf.int32)\n",
    "    input_pos_ids = tf.keras.layers.Input(shape=(text_max_len,), dtype=tf.int32)\n",
    "    embeds_1 = CLIPTextTransformer('conditioner.embedders.0.transformer.text_model')([input_word_ids, input_pos_ids])\n",
    "    \n",
    "    #TODO: Eventually migrate the Pytorch open_clip to here instead \n",
    "    #embeds_2 = CLIPTextTransformer('conditioner.embedders.1.model', True)([input_word_ids, input_pos_ids])\n",
    "    \n",
    "    #return tf.keras.models.Model([input_word_ids,input_pos_ids], tf.concat([embeds_1, embeds_2], 2))\n",
    "    return tf.keras.models.Model([input_word_ids, input_pos_ids], embeds_1)\n",
    "\n",
    "def get_unet():\n",
    "    context = tf.keras.layers.Input((text_max_len,2048))\n",
    "    t_emb = tf.keras.layers.Input((320,))\n",
    "    latent = tf.keras.layers.Input((128,128,4))\n",
    "    \n",
    "    # in_channels,\n",
    "    # out_channels,\n",
    "    # model_channels,\n",
    "    # attention_resolutions,\n",
    "    # num_res_blocks,\n",
    "    # channel_mult,\n",
    "    # num_heads\n",
    "    # num_head_channels\n",
    "    # context_dim,\n",
    "    unet = UNetModel(\"model.diffusion_model\", 4, 4, 320, [4, 2], 2, [1, 2, 4], -1, 64, 2048)\n",
    "    #unet = UNetModel(\"model.diffusion_model\", 4, 4, 320, [4, 2, 1], 2, [1, 2, 4, 4], 8, -1, 768)\n",
    " \n",
    "    return tf.keras.models.Model([latent, t_emb, context], unet([latent, t_emb, context]))\n",
    "    \n",
    "def get_decoder():\n",
    "    latent = tf.keras.layers.Input((128,128,4))\n",
    "    decoder = Decoder(\"first_stage_model.decoder\")\n",
    "    return tf.keras.models.Model(latent, decoder(latent))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bbccaa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = get_text_enc()\n",
    "unet = get_unet()\n",
    "decoder = get_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45156971-22c8-4b87-96e8-bc3511d750df",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conditioner.embedders.1.model.ln_final.bias\n",
      "conditioner.embedders.1.model.ln_final.weight\n",
      "conditioner.embedders.1.model.logit_scale\n",
      "conditioner.embedders.1.model.positional_embedding\n",
      "conditioner.embedders.1.model.text_projection\n",
      "conditioner.embedders.1.model.token_embedding.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.0.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.0.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.0.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.0.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.0.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.0.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.0.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.0.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.0.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.0.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.0.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.0.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.1.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.1.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.1.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.1.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.1.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.1.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.1.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.1.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.1.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.1.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.1.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.1.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.10.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.10.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.10.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.10.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.10.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.10.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.10.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.10.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.10.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.10.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.10.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.10.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.11.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.11.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.11.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.11.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.11.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.11.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.11.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.11.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.11.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.11.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.11.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.11.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.12.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.12.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.12.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.12.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.12.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.12.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.12.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.12.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.12.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.12.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.12.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.12.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.13.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.13.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.13.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.13.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.13.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.13.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.13.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.13.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.13.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.13.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.13.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.13.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.14.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.14.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.14.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.14.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.14.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.14.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.14.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.14.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.14.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.14.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.14.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.14.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.15.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.15.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.15.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.15.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.15.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.15.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.15.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.15.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.15.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.15.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.15.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.15.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.16.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.16.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.16.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.16.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.16.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.16.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.16.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.16.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.16.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.16.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.16.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.16.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.17.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.17.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.17.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.17.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.17.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.17.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.17.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.17.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.17.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.17.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.17.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.17.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.18.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.18.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.18.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.18.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.18.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.18.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.18.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.18.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.18.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.18.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.18.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.18.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.19.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.19.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.19.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.19.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.19.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.19.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.19.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.19.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.19.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.19.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.19.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.19.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.2.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.2.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.2.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.2.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.2.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.2.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.2.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.2.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.2.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.2.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.2.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.2.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.20.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.20.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.20.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.20.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.20.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.20.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.20.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.20.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.20.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.20.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.20.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.20.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.21.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.21.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.21.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.21.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.21.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.21.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.21.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.21.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.21.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.21.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.21.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.21.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.22.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.22.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.22.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.22.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.22.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.22.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.22.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.22.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.22.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.22.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.22.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.22.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.23.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.23.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.23.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.23.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.23.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.23.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.23.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.23.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.23.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.23.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.23.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.23.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.24.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.24.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.24.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.24.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.24.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.24.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.24.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.24.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.24.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.24.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.24.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.24.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.25.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.25.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.25.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.25.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.25.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.25.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.25.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.25.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.25.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.25.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.25.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.25.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.26.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.26.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.26.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.26.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.26.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.26.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.26.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.26.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.26.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.26.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.26.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.26.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.27.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.27.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.27.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.27.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.27.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.27.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.27.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.27.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.27.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.27.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.27.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.27.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.28.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.28.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.28.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.28.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.28.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.28.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.28.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.28.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.28.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.28.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.28.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.28.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.29.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.29.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.29.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.29.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.29.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.29.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.29.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.29.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.29.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.29.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.29.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.29.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.3.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.3.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.3.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.3.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.3.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.3.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.3.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.3.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.3.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.3.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.3.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.3.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.30.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.30.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.30.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.30.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.30.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.30.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.30.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.30.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.30.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.30.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.30.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.30.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.31.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.31.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.31.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.31.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.31.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.31.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.31.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.31.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.31.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.31.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.31.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.31.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.4.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.4.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.4.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.4.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.4.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.4.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.4.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.4.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.4.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.4.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.4.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.4.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.5.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.5.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.5.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.5.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.5.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.5.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.5.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.5.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.5.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.5.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.5.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.5.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.6.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.6.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.6.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.6.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.6.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.6.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.6.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.6.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.6.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.6.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.6.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.6.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.7.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.7.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.7.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.7.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.7.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.7.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.7.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.7.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.7.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.7.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.7.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.7.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.8.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.8.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.8.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.8.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.8.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.8.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.8.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.8.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.8.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.8.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.8.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.8.mlp.c_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.9.attn.in_proj_bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.9.attn.in_proj_weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.9.attn.out_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.9.attn.out_proj.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.9.ln_1.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.9.ln_1.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.9.ln_2.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.9.ln_2.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.9.mlp.c_fc.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.9.mlp.c_fc.weight\n",
      "conditioner.embedders.1.model.transformer.resblocks.9.mlp.c_proj.bias\n",
      "conditioner.embedders.1.model.transformer.resblocks.9.mlp.c_proj.weight\n"
     ]
    }
   ],
   "source": [
    "filtered = [x for x in weights['state_dict'].keys() if \"conditioner.embedders.1\" in x]\n",
    "print(*filtered, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d43f6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unet.save_weights(\"/tmp/unet.h5\")\n",
    "#decoder.save_weights(\"/tmp/decoder.h5\")\n",
    "#text_encoder.save_weights(\"/tmp/text_encoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c40e7dfb-2300-4b3b-9c77-75579eaa0d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"an astronaut riding a horse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0e19191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\James\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'visual_projection.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'logit_scale', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "#tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "model = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6f8b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt], padding=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d519a0cb-abfb-41fe-b2b7-deef1dcb9682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "model_openclip, _, _ = open_clip.create_model_and_transforms('ViT-bigG-14', pretrained='laion2b_s39b_b160k')\n",
    "tokenizer_openclip = open_clip.get_tokenizer('ViT-bigG-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6167db20-16ef-456c-9db5-410bde5dc771",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_openclip = tokenizer_openclip([prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7171475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed88ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos_ids = tf.convert_to_tensor(np.array(list(range(77)))[None].astype('int32'))\n",
    "#pos_ids = np.repeat(pos_ids, batch_size, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d05c7c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#phrase = inputs['input_ids'][0] + [49407]*(77-len(inputs['input_ids'][0]))\n",
    "\n",
    "#phrase = np.array(phrase)[None].astype(\"int32\")\n",
    "#phrase = np.repeat(phrase , batch_size, axis=0)\n",
    "#phrase = tf.convert_to_tensor(phrase)\n",
    "#context_clip = text_encoder([phrase])\n",
    "#context_clip.shape\n",
    "context_clip = model(**inputs)\n",
    "context_clip = context_clip.pooler_output.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c9a2f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = [49406, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407]\n",
    "phrase = np.array(phrase)[None].astype(\"int32\")\n",
    "phrase = np.repeat(phrase , batch_size, axis=0)\n",
    "\n",
    "#phrase = tf.convert_to_tensor(phrase)\n",
    "#unconditional_context_clip = text_encoder([phrase])\n",
    "#unconditional_context_clip.shape\n",
    "unconditional_context_clip = model(**tokenizer([\"\"], padding=True, return_tensors=\"pt\"))\n",
    "unconditional_context_clip = unconditional_context_clip.pooler_output.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "038e59d7-75bc-4204-88db-852ef8c8f796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1280)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_openclip = model_openclip.encode_text(inputs_openclip).detach().numpy()\n",
    "context_openclip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8593a22d-a509-4389-9d89-5a4144c40693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1280)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = [49406, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407]\n",
    "phrase = np.array(phrase)[None].astype(\"int32\")\n",
    "phrase = np.repeat(phrase , batch_size, axis=0)\n",
    "\n",
    "phrase = torch.from_numpy(phrase)\n",
    "unconditional_context_openclip = model_openclip.encode_text(phrase).detach().numpy()\n",
    "unconditional_context_openclip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e09c1407-fa24-415f-a2e5-f5be5bcfa281",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = tf.concat([context_clip, context_openclip], 1)\n",
    "unconditional_context = tf.concat([unconditional_context_clip, unconditional_context_openclip], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "14a5b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_embedding(timesteps, dim=320, max_period=10000):\n",
    "  half = dim // 2\n",
    "  freqs = np.exp(-math.log(max_period) * np.arange(0, half, dtype=np.float32) / half)\n",
    "  print(freqs.shape)\n",
    "  args = np.array(timesteps) * freqs\n",
    "  embedding = np.concatenate([np.cos(args), np.sin(args)])\n",
    "  return (embedding).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b2be335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_output(latent, t):\n",
    "    # put into diffuser\n",
    "    timesteps = np.array([t])\n",
    "    t_emb = tf.convert_to_tensor(timestep_embedding(timesteps))\n",
    "    t_emb = np.repeat(t_emb , batch_size, axis=0)\n",
    "\n",
    "    unconditional_latent = unet([latent, t_emb, unconditional_context])\n",
    "    latent = unet([latent, t_emb, context])\n",
    "    unconditional_guidance_scale = 7.5\n",
    "    e_t = unconditional_latent + unconditional_guidance_scale * (latent - unconditional_latent)\n",
    "    return e_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "722af063",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running for [1, 41, 81, 121, 161, 201, 241, 281, 321, 361, 401, 441, 481, 521, 561, 601, 641, 681, 721, 761, 801, 841, 881, 921, 961] timesteps\n"
     ]
    }
   ],
   "source": [
    "TIMESTEPS = int(os.getenv(\"TIMESTEPS\", \"25\"))\n",
    "timesteps = list(np.arange(1, 1000, 1000//TIMESTEPS))\n",
    "print(f\"running for {timesteps} timesteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "57d9816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_start=0.00085\n",
    "linear_end=0.0120\n",
    "num_timesteps=1000\n",
    "betas = tf.linspace(tf.sqrt(linear_start), tf.sqrt(linear_end), num_timesteps) ** 2\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = np.cumprod(alphas, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0a698e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(alphas_cumprod.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "60c30cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [alphas_cumprod[t] for t in timesteps]\n",
    "alphas_prev = [1.0] + alphas[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "681259b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_prev_and_pred_x0(x, e_t, index):\n",
    "    temperature = 1\n",
    "    a_t, a_prev = alphas[index], alphas_prev[index]\n",
    "    sigma_t = 0\n",
    "    sqrt_one_minus_at = math.sqrt(1-a_t)\n",
    "    #print(a_t, a_prev, sigma_t, sqrt_one_minus_at)\n",
    "\n",
    "    pred_x0 = (x - sqrt_one_minus_at * e_t) / math.sqrt(a_t)\n",
    "\n",
    "    # direction pointing to x_t\n",
    "    dir_xt = math.sqrt(1. - a_prev - sigma_t**2) * e_t\n",
    "    noise = sigma_t *  tf.random.normal(x.shape) * temperature\n",
    "\n",
    "    x_prev = math.sqrt(a_prev) * pred_x0 + dir_xt #+ noise\n",
    "    return x_prev, pred_x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9279713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = tf.random.normal((batch_size,64,64,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3e622e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "332f01cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24 961:   0%|                                                                                  | 0/25 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 2 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 77, 2048), found shape=(1, 2048)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, timestep \u001b[38;5;129;01min\u001b[39;00m (t\u001b[38;5;241m:=\u001b[39mtqdm(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(timesteps))[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])):\n\u001b[0;32m      2\u001b[0m     t\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%3d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%3d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (index, timestep))\n\u001b[1;32m----> 3\u001b[0m     e_t \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     x_prev, pred_x0 \u001b[38;5;241m=\u001b[39m get_x_prev_and_pred_x0(latent, e_t, index)\n\u001b[0;32m      5\u001b[0m     latent \u001b[38;5;241m=\u001b[39m x_prev\n",
      "Cell \u001b[1;32mIn[66], line 7\u001b[0m, in \u001b[0;36mget_model_output\u001b[1;34m(latent, t)\u001b[0m\n\u001b[0;32m      4\u001b[0m t_emb \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(timestep_embedding(timesteps))\n\u001b[0;32m      5\u001b[0m t_emb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(t_emb , batch_size, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m unconditional_latent \u001b[38;5;241m=\u001b[39m \u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlatent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munconditional_context\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m latent \u001b[38;5;241m=\u001b[39m unet([latent, t_emb, context])\n\u001b[0;32m      9\u001b[0m unconditional_guidance_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7.5\u001b[39m\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.13\\lib\\site-packages\\keras\\src\\engine\\input_spec.py:298\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 298\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    299\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    300\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplay_shape(x\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 2 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 77, 2048), found shape=(1, 2048)"
     ]
    }
   ],
   "source": [
    "for index, timestep in (t:=tqdm(list(enumerate(timesteps))[::-1])):\n",
    "    t.set_description(\"%3d %3d\" % (index, timestep))\n",
    "    e_t = get_model_output(latent, timestep)\n",
    "    x_prev, pred_x0 = get_x_prev_and_pred_x0(latent, e_t, index)\n",
    "    latent = x_prev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f061f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = decoder(latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac4713",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.clip((decoded[0].numpy()+1)/2, 0 , 1 ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
